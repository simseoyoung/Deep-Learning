{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_AE1.ipynb",
      "provenance": [],
      "mount_file_id": "1Y4l845EtN5uK1z7gQnqRvfNaLm2PsIog",
      "authorship_tag": "ABX9TyOMW6Z7JuyZj7nl05BeavOi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simseoyoung/Deep-Learning/blob/main/Project/LSTM_AE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Compression using autoencoder\n",
        "\n",
        "- ECE 659/493"
      ],
      "metadata": {
        "id": "aFsC6FTq6RjA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X0vJ9nH9muhw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdnx-9bBmxrt",
        "outputId": "1781a9ca-fce2-4c1d-f182-ce1ed9e4d342"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "ZYExBg44myQ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Data\n",
        "- Pump sensor data (https://www.kaggle.com/datasets/nphantawee/pump-sensor-data) \n",
        "- April,2018~August,2018 for minutes\n",
        "- 52 sensors\n",
        "- 7 error in it (contains label - Normal, Broken, Recovering)"
      ],
      "metadata": {
        "id": "8L8qIsJL6Kpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_data_url = \"/content/drive/MyDrive/ECE 453 project/sensor.csv\"\n",
        "df = pd.read_csv(train_data_url)\n",
        "\n",
        "# drop unecessary column\n",
        "df = df.iloc[: , 1:] \n",
        "del df['timestamp']\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "hiUg5QcXmz9b",
        "outputId": "b32d596c-4c85-42fa-880b-da41c4429c15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        sensor_00  sensor_01  sensor_02  sensor_03   sensor_04  sensor_05  \\\n",
              "0        2.465394   47.09201  53.211800  46.310760  634.375000   76.45975   \n",
              "1        2.465394   47.09201  53.211800  46.310760  634.375000   76.45975   \n",
              "2        2.444734   47.35243  53.211800  46.397570  638.888900   73.54598   \n",
              "3        2.460474   47.09201  53.168400  46.397568  628.125000   76.98898   \n",
              "4        2.445718   47.13541  53.211800  46.397568  636.458300   76.58897   \n",
              "...           ...        ...        ...        ...         ...        ...   \n",
              "220315   2.407350   47.69965  50.520830  43.142361  634.722229   64.59095   \n",
              "220316   2.400463   47.69965  50.564240  43.142361  630.902771   65.83363   \n",
              "220317   2.396528   47.69965  50.520830  43.142361  625.925903   67.29445   \n",
              "220318   2.406366   47.69965  50.520832  43.142361  635.648100   65.09175   \n",
              "220319   2.396528   47.69965  50.520832  43.142361  639.814800   65.45634   \n",
              "\n",
              "        sensor_06  sensor_07  sensor_08  sensor_09  ...  sensor_43  sensor_44  \\\n",
              "0        13.41146   16.13136   15.56713   15.05353  ...   41.92708  39.641200   \n",
              "1        13.41146   16.13136   15.56713   15.05353  ...   41.92708  39.641200   \n",
              "2        13.32465   16.03733   15.61777   15.01013  ...   41.66666  39.351852   \n",
              "3        13.31742   16.24711   15.69734   15.08247  ...   40.88541  39.062500   \n",
              "4        13.35359   16.21094   15.69734   15.08247  ...   41.40625  38.773150   \n",
              "...           ...        ...        ...        ...  ...        ...        ...   \n",
              "220315   15.11863   16.65220   15.65393   15.16204  ...   38.28125  68.287030   \n",
              "220316   15.15480   16.70284   15.65393   15.11863  ...   38.28125  66.840280   \n",
              "220317   15.08970   16.70284   15.69734   15.11863  ...   39.06250  65.393520   \n",
              "220318   15.11863   16.56539   15.74074   15.11863  ...   40.62500  64.236110   \n",
              "220319   15.11863   16.65220   15.65393   15.01013  ...   41.40625  62.789350   \n",
              "\n",
              "        sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  sensor_50  \\\n",
              "0        65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
              "1        65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
              "2        65.39352   51.21528  38.194443   155.9606   67.12963   241.3194   \n",
              "3        64.81481   51.21528  38.194440   155.9606   66.84028   240.4514   \n",
              "4        65.10416   51.79398  38.773150   158.2755   66.55093   242.1875   \n",
              "...           ...        ...        ...        ...        ...        ...   \n",
              "220315   52.37268   48.32176  41.087960   212.3843  153.64580        NaN   \n",
              "220316   50.63657   48.03241  40.798610   213.8310  156.25000        NaN   \n",
              "220317   48.90046   48.03241  40.798610   217.3032  155.38190        NaN   \n",
              "220318   47.74306   48.32176  40.509258   222.5116  153.93520        NaN   \n",
              "220319   46.29630   48.90046  40.219910   227.4306  150.46300        NaN   \n",
              "\n",
              "        sensor_51  machine_status  \n",
              "0        201.3889          NORMAL  \n",
              "1        201.3889          NORMAL  \n",
              "2        203.7037          NORMAL  \n",
              "3        203.1250          NORMAL  \n",
              "4        201.3889          NORMAL  \n",
              "...           ...             ...  \n",
              "220315   231.1921          NORMAL  \n",
              "220316   231.1921          NORMAL  \n",
              "220317   232.0602          NORMAL  \n",
              "220318   234.0856          NORMAL  \n",
              "220319   234.0856          NORMAL  \n",
              "\n",
              "[220320 rows x 53 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a2923d1-1a24-4011-9bde-df838684b43c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>...</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>machine_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>...</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>...</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.35243</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>...</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.168400</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>...</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.13541</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>...</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220315</th>\n",
              "      <td>2.407350</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>634.722229</td>\n",
              "      <td>64.59095</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>...</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>68.287030</td>\n",
              "      <td>52.37268</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>41.087960</td>\n",
              "      <td>212.3843</td>\n",
              "      <td>153.64580</td>\n",
              "      <td>NaN</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220316</th>\n",
              "      <td>2.400463</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.564240</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>630.902771</td>\n",
              "      <td>65.83363</td>\n",
              "      <td>15.15480</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>...</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>66.840280</td>\n",
              "      <td>50.63657</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>213.8310</td>\n",
              "      <td>156.25000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220317</th>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>625.925903</td>\n",
              "      <td>67.29445</td>\n",
              "      <td>15.08970</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>...</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>65.393520</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>217.3032</td>\n",
              "      <td>155.38190</td>\n",
              "      <td>NaN</td>\n",
              "      <td>232.0602</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220318</th>\n",
              "      <td>2.406366</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>635.648100</td>\n",
              "      <td>65.09175</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.56539</td>\n",
              "      <td>15.74074</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>...</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>64.236110</td>\n",
              "      <td>47.74306</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>40.509258</td>\n",
              "      <td>222.5116</td>\n",
              "      <td>153.93520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220319</th>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>639.814800</td>\n",
              "      <td>65.45634</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>...</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>62.789350</td>\n",
              "      <td>46.29630</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>40.219910</td>\n",
              "      <td>227.4306</td>\n",
              "      <td>150.46300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220320 rows Ã— 53 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a2923d1-1a24-4011-9bde-df838684b43c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a2923d1-1a24-4011-9bde-df838684b43c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a2923d1-1a24-4011-9bde-df838684b43c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "# change data type ( for sensor to numeric)\n",
        "for var_index in [item for item in df.columns if 'sensor_' in item]:\n",
        "  df[var_index] = pd.to_numeric(df[var_index], errors = 'coerce')\n",
        "\n",
        "# find Null\n",
        "(df.isnull().sum()/len(df)).plot.bar(figsize=(18, 8), colormap='Paired')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "cvPjVmoQm1-z",
        "outputId": "2a689e9e-1717-4fca-88bf-c5759d19c779"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f43586f59d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since sensor 15 is all null, sensor 50 is 50% null, delete\n",
        "# other sensors with null, use previous time's data for null\n",
        "\n",
        "# delete duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# delete sensor 15, 50\n",
        "del df['sensor_15']\n",
        "del df['sensor_50']\n",
        "\n",
        "# fill null for previous data\n",
        "df = df.fillna(method = 'ffill')"
      ],
      "metadata": {
        "id": "E9gF5OXwnJIU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there is still nan (False = No, True = Yes)\n",
        "\n",
        "check_for_nan = df.isnull().values.any()\n",
        "print (check_for_nan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1-FFnderqZO",
        "outputId": "94fa09ca-6049-45ca-9e09-e4d000c15856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change string to the number (normal = 1, others = 0)\n",
        "Status = { \"NORMAL\": 1, \"BROKEN\": 0, \"RECOVERING\":0}\n",
        "state = df[\"machine_status\"].replace(Status)\n",
        "df['machine_status']=state\n",
        "\n",
        "df['machine_status']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiUYA0hunK8X",
        "outputId": "077b69e3-b773-45c0-bf94-c76826c8ece5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "2         1\n",
              "3         1\n",
              "4         1\n",
              "5         1\n",
              "         ..\n",
              "220315    1\n",
              "220316    1\n",
              "220317    1\n",
              "220318    1\n",
              "220319    1\n",
              "Name: machine_status, Length: 217444, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split x, y\n",
        "input_x = df.drop('machine_status', axis=1).values\n",
        "input_y = df['machine_status'].values\n",
        "\n",
        "n_features = input_x.shape[1]"
      ],
      "metadata": {
        "id": "XOtiYoN3nNYr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM needs 3dim data shape, change it to 3 dim using timestep\n",
        "\n",
        "def temporalize(X, y, timesteps):\n",
        "\toutput_X = []\n",
        "\toutput_y = []\n",
        "\tfor i in range(len(X) - timesteps - 1):\n",
        "\t\tt = []\n",
        "\t\tfor j in range(1, timesteps + 1):\n",
        "\t\t\t# Gather the past records upto the lookback period\n",
        "\t\t\tt.append(X[[(i + j + 1)], :])\n",
        "\t\toutput_X.append(t)\n",
        "\t\toutput_y.append(y[i + timesteps + 1])\n",
        "\treturn np.squeeze(np.array(output_X)), np.array(output_y)"
      ],
      "metadata": {
        "id": "MxW_uknAnXgz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define timestep as 5 min\n",
        "timesteps = 5\n",
        "# Temporalize\n",
        "x, y = temporalize(input_x, input_y, timesteps)\n",
        "print(x.shape) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj9Eth7LnZYb",
        "outputId": "e2cf7b74-32e3-47f6-a2fe-29634998890e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(217438, 5, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, valid, and test \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "print('length of x_train : ',len(x_train))  \n",
        "print('length of x_valid : ',len(x_valid))  \n",
        "print('length of x_test : ',len(x_test))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb1AAhIynxjl",
        "outputId": "5c115239-0d35-40fd-e1ff-6f22e306a712"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of x_train :  139160\n",
            "length of x_valid :  34790\n",
            "length of x_test :  43488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For training the autoencoder, split 0 / 1\n",
        "# use only normal data for training\n",
        "x_train_y0 = x_train[y_train == 0]\n",
        "x_train_y1 = x_train[y_train == 1]\n",
        "\n",
        "x_valid_y0 = x_valid[y_valid == 0]\n",
        "x_valid_y1 = x_valid[y_valid == 1]\n",
        "print(x_train_y0.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cckehy_3n16o",
        "outputId": "3e9a3655-419f-4613-9e9e-90809e114cd1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9100, 5, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use flatten since scaler needs 2D\n",
        "def flatten(X):\n",
        "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
        "    for i in range(X.shape[0]):\n",
        "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
        "    return(flattened_X)\n",
        "\n",
        "def scale(X, scaler):\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
        "        \n",
        "    return X"
      ],
      "metadata": {
        "id": "ygCK6qDJn51y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization for different data characteristics\n",
        "scaler = StandardScaler().fit(flatten(x_train_y1))\n",
        "\n",
        "x_train_y1_scaled = scale(x_train_y1, scaler)\n",
        "x_valid_scaled = scale(x_valid, scaler)\n",
        "x_valid_y1_scaled = scale(x_valid_y1, scaler)\n",
        "x_test_scaled = scale(x_test, scaler)"
      ],
      "metadata": {
        "id": "-wiwN2Bzn64S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "epochs = 200\n",
        "batch = 128\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "VPc9u9P8oc5A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Model\n",
        "- LSTM Autoencdoer for sequence (timeseries data)"
      ],
      "metadata": {
        "id": "6a4bVrPOmznE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # define model\n",
        "lstm_ae = keras.models.Sequential()\n",
        "# Encoder\n",
        "lstm_ae.add(keras.layers.LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
        "lstm_ae.add(keras.layers.LSTM(16, activation='relu', return_sequences=False))\n",
        "lstm_ae.add(keras.layers.RepeatVector(timesteps))\n",
        "# Decoder\n",
        "lstm_ae.add(keras.layers.LSTM(16, activation='relu', return_sequences=True))\n",
        "lstm_ae.add(keras.layers.LSTM(32, activation='relu', return_sequences=True))\n",
        "lstm_ae.add(keras.layers.TimeDistributed(keras.layers.Dense(n_features)))\n",
        "\n",
        "lstm_ae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnOmG7qEodnb",
        "outputId": "77d974d2-70ff-4d6f-a2c7-6ebe7a5dc733"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 5, 32)             10624     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 5, 16)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 5, 16)             2112      \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 32)             6272      \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 5, 50)            1650      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,794\n",
            "Trainable params: 23,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile\n",
        "lstm_ae.compile(loss='mse', optimizer=keras.optimizers.Adam(lr))\n",
        "\n",
        "# fit\n",
        "history = lstm_ae.fit(x_train_y1_scaled, x_train_y1_scaled,\n",
        "                     epochs=epochs, batch_size=batch,\n",
        "                     validation_data=(x_valid_y1_scaled, x_valid_y1_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEn2o3dkomsW",
        "outputId": "48f95f2e-6f73-4643-d6db-b2b568d71cf8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1017/1017 [==============================] - 27s 23ms/step - loss: 0.3516 - val_loss: 0.2060\n",
            "Epoch 2/200\n",
            "1017/1017 [==============================] - 27s 26ms/step - loss: 0.1774 - val_loss: 0.1605\n",
            "Epoch 3/200\n",
            "1017/1017 [==============================] - 23s 23ms/step - loss: 0.1487 - val_loss: 0.1405\n",
            "Epoch 4/200\n",
            "1017/1017 [==============================] - 20s 19ms/step - loss: 0.1332 - val_loss: 0.1267\n",
            "Epoch 5/200\n",
            "1017/1017 [==============================] - 19s 18ms/step - loss: 0.1222 - val_loss: 0.1174\n",
            "Epoch 6/200\n",
            "1017/1017 [==============================] - 20s 20ms/step - loss: 0.1139 - val_loss: 0.1088\n",
            "Epoch 7/200\n",
            "1017/1017 [==============================] - 29s 28ms/step - loss: 0.1072 - val_loss: 0.1053\n",
            "Epoch 8/200\n",
            "1017/1017 [==============================] - 17s 17ms/step - loss: 0.1044 - val_loss: 0.1012\n",
            "Epoch 9/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.1012 - val_loss: 0.1008\n",
            "Epoch 10/200\n",
            "1017/1017 [==============================] - 18s 18ms/step - loss: 0.0986 - val_loss: 0.1145\n",
            "Epoch 11/200\n",
            "1017/1017 [==============================] - 20s 20ms/step - loss: 0.0974 - val_loss: 0.0981\n",
            "Epoch 12/200\n",
            "1017/1017 [==============================] - 20s 20ms/step - loss: 0.0947 - val_loss: 0.0945\n",
            "Epoch 13/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0930 - val_loss: 0.0931\n",
            "Epoch 14/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0918 - val_loss: 0.0900\n",
            "Epoch 15/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0905 - val_loss: 0.0898\n",
            "Epoch 16/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0900 - val_loss: 0.0896\n",
            "Epoch 17/200\n",
            "1017/1017 [==============================] - 24s 24ms/step - loss: 0.0880 - val_loss: 0.0881\n",
            "Epoch 18/200\n",
            "1017/1017 [==============================] - 25s 25ms/step - loss: 0.0874 - val_loss: 0.0889\n",
            "Epoch 19/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0872 - val_loss: 0.0871\n",
            "Epoch 20/200\n",
            "1017/1017 [==============================] - 24s 24ms/step - loss: 0.0858 - val_loss: 0.0873\n",
            "Epoch 21/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0850 - val_loss: 0.0857\n",
            "Epoch 22/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0844 - val_loss: 0.0839\n",
            "Epoch 23/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0836 - val_loss: 0.0834\n",
            "Epoch 24/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0828 - val_loss: 0.0857\n",
            "Epoch 25/200\n",
            "1017/1017 [==============================] - 18s 17ms/step - loss: 0.0828 - val_loss: 0.0824\n",
            "Epoch 26/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0815 - val_loss: 0.0819\n",
            "Epoch 27/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0814 - val_loss: 0.0831\n",
            "Epoch 28/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0811 - val_loss: 0.0848\n",
            "Epoch 29/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0804 - val_loss: 0.0797\n",
            "Epoch 30/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0799 - val_loss: 0.0802\n",
            "Epoch 31/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0794 - val_loss: 0.0799\n",
            "Epoch 32/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0794 - val_loss: 0.0814\n",
            "Epoch 33/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0787 - val_loss: 0.0786\n",
            "Epoch 34/200\n",
            "1017/1017 [==============================] - 22s 22ms/step - loss: 0.0781 - val_loss: 0.0796\n",
            "Epoch 35/200\n",
            "1017/1017 [==============================] - 21s 20ms/step - loss: 0.0780 - val_loss: 0.0971\n",
            "Epoch 36/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0788 - val_loss: 0.0771\n",
            "Epoch 37/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0769 - val_loss: 0.0774\n",
            "Epoch 38/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0766 - val_loss: 0.0766\n",
            "Epoch 39/200\n",
            "1017/1017 [==============================] - 19s 19ms/step - loss: 0.0760 - val_loss: 0.0788\n",
            "Epoch 40/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0758 - val_loss: 0.0763\n",
            "Epoch 41/200\n",
            "1017/1017 [==============================] - 21s 21ms/step - loss: 0.0767 - val_loss: 0.0770\n",
            "Epoch 42/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0752 - val_loss: 0.0752\n",
            "Epoch 43/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0749 - val_loss: 0.0748\n",
            "Epoch 44/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0748 - val_loss: 0.0748\n",
            "Epoch 45/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0757 - val_loss: 0.0750\n",
            "Epoch 46/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0739 - val_loss: 0.0778\n",
            "Epoch 47/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0744 - val_loss: 0.0779\n",
            "Epoch 48/200\n",
            "1017/1017 [==============================] - 17s 17ms/step - loss: 0.0738 - val_loss: 0.0739\n",
            "Epoch 49/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0736 - val_loss: 0.0744\n",
            "Epoch 50/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0732 - val_loss: 0.0744\n",
            "Epoch 51/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0731 - val_loss: 0.0742\n",
            "Epoch 52/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0736 - val_loss: 0.0726\n",
            "Epoch 53/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0721 - val_loss: 0.0725\n",
            "Epoch 54/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0710 - val_loss: 0.0720\n",
            "Epoch 55/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0710 - val_loss: 0.0731\n",
            "Epoch 56/200\n",
            "1017/1017 [==============================] - 17s 17ms/step - loss: 0.0707 - val_loss: 0.0730\n",
            "Epoch 57/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0701 - val_loss: 0.0698\n",
            "Epoch 58/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0708 - val_loss: 0.0759\n",
            "Epoch 59/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0704 - val_loss: 0.0714\n",
            "Epoch 60/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0697 - val_loss: 0.0720\n",
            "Epoch 61/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0693 - val_loss: 0.0690\n",
            "Epoch 62/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0689 - val_loss: 0.0689\n",
            "Epoch 63/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0704 - val_loss: 0.0706\n",
            "Epoch 64/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0692 - val_loss: 0.0694\n",
            "Epoch 65/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0686 - val_loss: 0.0686\n",
            "Epoch 66/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0682 - val_loss: 0.0681\n",
            "Epoch 67/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0694 - val_loss: 0.0721\n",
            "Epoch 68/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0687 - val_loss: 0.0691\n",
            "Epoch 69/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0685 - val_loss: 0.0675\n",
            "Epoch 70/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0678 - val_loss: 0.0677\n",
            "Epoch 71/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0673 - val_loss: 0.0681\n",
            "Epoch 72/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0681 - val_loss: 0.0675\n",
            "Epoch 73/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0675 - val_loss: 0.0693\n",
            "Epoch 74/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0671 - val_loss: 0.0679\n",
            "Epoch 75/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0670 - val_loss: 0.0672\n",
            "Epoch 76/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0668 - val_loss: 0.0689\n",
            "Epoch 77/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0670 - val_loss: 0.0671\n",
            "Epoch 78/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0673 - val_loss: 0.0663\n",
            "Epoch 79/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0669 - val_loss: 0.0689\n",
            "Epoch 80/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0672 - val_loss: 0.0727\n",
            "Epoch 81/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0662 - val_loss: 0.0689\n",
            "Epoch 82/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0671 - val_loss: 0.0677\n",
            "Epoch 83/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0661 - val_loss: 0.0663\n",
            "Epoch 84/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0658 - val_loss: 0.0673\n",
            "Epoch 85/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0659 - val_loss: 0.0662\n",
            "Epoch 86/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0656 - val_loss: 0.0657\n",
            "Epoch 87/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0658 - val_loss: 0.0667\n",
            "Epoch 88/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0657 - val_loss: 0.0678\n",
            "Epoch 89/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0657 - val_loss: 0.0685\n",
            "Epoch 90/200\n",
            "1017/1017 [==============================] - 17s 17ms/step - loss: 0.0652 - val_loss: 0.0681\n",
            "Epoch 91/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0659 - val_loss: 0.0656\n",
            "Epoch 92/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0663 - val_loss: 0.0651\n",
            "Epoch 93/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0646 - val_loss: 0.0675\n",
            "Epoch 94/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0651 - val_loss: 0.0650\n",
            "Epoch 95/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0650 - val_loss: 0.0644\n",
            "Epoch 96/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0644 - val_loss: 0.0651\n",
            "Epoch 97/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0649 - val_loss: 0.0634\n",
            "Epoch 98/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0640 - val_loss: 0.0645\n",
            "Epoch 99/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 100/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0637 - val_loss: 0.0643\n",
            "Epoch 101/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0630 - val_loss: 0.0633\n",
            "Epoch 102/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0628 - val_loss: 0.0648\n",
            "Epoch 103/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0626 - val_loss: 0.0631\n",
            "Epoch 104/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0623 - val_loss: 0.0628\n",
            "Epoch 105/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0622 - val_loss: 0.0623\n",
            "Epoch 106/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0617 - val_loss: 0.0623\n",
            "Epoch 107/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0620 - val_loss: 0.0630\n",
            "Epoch 108/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0626 - val_loss: 0.0630\n",
            "Epoch 109/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0615 - val_loss: 0.0610\n",
            "Epoch 110/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0625 - val_loss: 0.0652\n",
            "Epoch 111/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0626 - val_loss: 0.0632\n",
            "Epoch 112/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0610 - val_loss: 0.0624\n",
            "Epoch 113/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0634 - val_loss: 0.0623\n",
            "Epoch 114/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0606 - val_loss: 0.0620\n",
            "Epoch 115/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0610 - val_loss: 0.0613\n",
            "Epoch 116/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0630 - val_loss: 0.0681\n",
            "Epoch 117/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0613 - val_loss: 0.0617\n",
            "Epoch 118/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0620 - val_loss: 0.0618\n",
            "Epoch 119/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0616 - val_loss: 0.0611\n",
            "Epoch 120/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0606 - val_loss: 0.0607\n",
            "Epoch 121/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0605 - val_loss: 0.0643\n",
            "Epoch 122/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0609 - val_loss: 0.0604\n",
            "Epoch 123/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0603 - val_loss: 0.0606\n",
            "Epoch 124/200\n",
            "1017/1017 [==============================] - 17s 17ms/step - loss: 0.0603 - val_loss: 0.0628\n",
            "Epoch 125/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0602 - val_loss: 0.0611\n",
            "Epoch 126/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0601 - val_loss: 0.0607\n",
            "Epoch 127/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0599 - val_loss: 0.0608\n",
            "Epoch 128/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0600 - val_loss: 0.0604\n",
            "Epoch 129/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0598 - val_loss: 0.0615\n",
            "Epoch 130/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0596 - val_loss: 0.0601\n",
            "Epoch 131/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0594 - val_loss: 0.0596\n",
            "Epoch 132/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0592 - val_loss: 0.0601\n",
            "Epoch 133/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0589 - val_loss: 0.0580\n",
            "Epoch 134/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0584 - val_loss: 0.0596\n",
            "Epoch 135/200\n",
            "1017/1017 [==============================] - 15s 15ms/step - loss: 0.0584 - val_loss: 0.0584\n",
            "Epoch 136/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0588 - val_loss: 0.0585\n",
            "Epoch 137/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0592 - val_loss: 0.0595\n",
            "Epoch 138/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0579 - val_loss: 0.0580\n",
            "Epoch 139/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0575 - val_loss: 0.0590\n",
            "Epoch 140/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0591\n",
            "Epoch 141/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0578 - val_loss: 0.0589\n",
            "Epoch 142/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0574 - val_loss: 0.0570\n",
            "Epoch 143/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0585 - val_loss: 0.0579\n",
            "Epoch 144/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0576 - val_loss: 0.0573\n",
            "Epoch 145/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0574 - val_loss: 0.0571\n",
            "Epoch 146/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0569 - val_loss: 0.0582\n",
            "Epoch 147/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0589\n",
            "Epoch 148/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0572 - val_loss: 0.0578\n",
            "Epoch 149/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0593\n",
            "Epoch 150/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0573 - val_loss: 0.0577\n",
            "Epoch 151/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0580 - val_loss: 0.0577\n",
            "Epoch 152/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0570 - val_loss: 0.0571\n",
            "Epoch 153/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0571 - val_loss: 0.0571\n",
            "Epoch 154/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0577 - val_loss: 0.0573\n",
            "Epoch 155/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0565 - val_loss: 0.0576\n",
            "Epoch 156/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0569 - val_loss: 0.0605\n",
            "Epoch 157/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0570 - val_loss: 0.0582\n",
            "Epoch 158/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0563 - val_loss: 0.0582\n",
            "Epoch 159/200\n",
            "1017/1017 [==============================] - 18s 17ms/step - loss: 0.0566 - val_loss: 0.0598\n",
            "Epoch 160/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0567 - val_loss: 0.0566\n",
            "Epoch 161/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0560 - val_loss: 0.0575\n",
            "Epoch 162/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0568 - val_loss: 0.0571\n",
            "Epoch 163/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0571 - val_loss: 0.0568\n",
            "Epoch 164/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0567 - val_loss: 0.0565\n",
            "Epoch 165/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0563 - val_loss: 0.0572\n",
            "Epoch 166/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0561 - val_loss: 0.0566\n",
            "Epoch 167/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0562 - val_loss: 0.0579\n",
            "Epoch 168/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0564 - val_loss: 0.0572\n",
            "Epoch 169/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0567 - val_loss: 0.0579\n",
            "Epoch 170/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0589 - val_loss: 0.0593\n",
            "Epoch 171/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0571 - val_loss: 0.0565\n",
            "Epoch 172/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0561 - val_loss: 0.0576\n",
            "Epoch 173/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0564 - val_loss: 0.0579\n",
            "Epoch 174/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0567 - val_loss: 0.0576\n",
            "Epoch 175/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0560 - val_loss: 0.0563\n",
            "Epoch 176/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0557 - val_loss: 0.0557\n",
            "Epoch 177/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0563 - val_loss: 0.0590\n",
            "Epoch 178/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0563 - val_loss: 0.0563\n",
            "Epoch 179/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0564 - val_loss: 0.0558\n",
            "Epoch 180/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0565 - val_loss: 0.0566\n",
            "Epoch 181/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0556 - val_loss: 0.0556\n",
            "Epoch 182/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0559 - val_loss: 0.0582\n",
            "Epoch 183/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0555 - val_loss: 0.0624\n",
            "Epoch 184/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0566 - val_loss: 0.0551\n",
            "Epoch 185/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0553 - val_loss: 0.0572\n",
            "Epoch 186/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0554 - val_loss: 0.0603\n",
            "Epoch 187/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0566 - val_loss: 0.0570\n",
            "Epoch 188/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0564 - val_loss: 0.0555\n",
            "Epoch 189/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0549 - val_loss: 0.0560\n",
            "Epoch 190/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0553 - val_loss: 0.0563\n",
            "Epoch 191/200\n",
            "1017/1017 [==============================] - 17s 16ms/step - loss: 0.0551 - val_loss: 0.0563\n",
            "Epoch 192/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0554 - val_loss: 0.0567\n",
            "Epoch 193/200\n",
            "1017/1017 [==============================] - 18s 18ms/step - loss: 0.0557 - val_loss: 0.0555\n",
            "Epoch 194/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0558 - val_loss: 0.0738\n",
            "Epoch 195/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0564 - val_loss: 0.0562\n",
            "Epoch 196/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0550 - val_loss: 0.0554\n",
            "Epoch 197/200\n",
            "1017/1017 [==============================] - 16s 15ms/step - loss: 0.0553 - val_loss: 0.0554\n",
            "Epoch 198/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0556 - val_loss: 0.0560\n",
            "Epoch 199/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0550 - val_loss: 0.0587\n",
            "Epoch 200/200\n",
            "1017/1017 [==============================] - 16s 16ms/step - loss: 0.0556 - val_loss: 0.0552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "plt.plot(history.history['loss'], 'b', label='train loss')\n",
        "plt.plot(history.history['val_loss'], 'g', label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_9s1qomsv9Lc",
        "outputId": "aae19a5a-f5c5-4eda-809d-28790ea24a7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrate reconstruction\n",
        "x_predict = lstm_ae.predict(x_test_scaled, verbose=0)\n",
        "print('---Predicted---')\n",
        "print(np.round(x_predict,3))\n",
        "print('---Actual---')\n",
        "print(np.round(x_test_scaled, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZGlft4T0AMA",
        "outputId": "1c48e917-b087-4e2e-bfc5-fa7cf5900cb6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Predicted---\n",
            "[[[ 0.2   -0.461 -0.483 ... -1.201 -0.689 -0.331]\n",
            "  [ 0.252 -0.422 -0.457 ... -1.347 -0.738 -0.332]\n",
            "  [ 0.264 -0.407 -0.437 ... -1.345 -0.735 -0.325]\n",
            "  [ 0.271 -0.396 -0.429 ... -1.367 -0.738 -0.327]\n",
            "  [ 0.273 -0.392 -0.427 ... -1.37  -0.736 -0.327]]\n",
            "\n",
            " [[ 0.355  0.613  0.476 ...  1.175  0.722 -0.146]\n",
            "  [ 0.367  0.498  0.437 ...  1.218  0.661 -0.169]\n",
            "  [ 0.357  0.553  0.477 ...  1.257  0.644 -0.163]\n",
            "  [ 0.363  0.586  0.496 ...  1.26   0.665 -0.163]\n",
            "  [ 0.362  0.585  0.497 ...  1.269  0.694 -0.161]]\n",
            "\n",
            " [[-0.642  1.125 -0.103 ... -1.274 -0.31  -0.441]\n",
            "  [-0.819  1.106 -0.159 ... -1.38  -0.329 -0.407]\n",
            "  [-0.809  1.11  -0.185 ... -1.407 -0.335 -0.392]\n",
            "  [-0.798  1.104 -0.185 ... -1.444 -0.339 -0.39 ]\n",
            "  [-0.787  1.109 -0.186 ... -1.48  -0.34  -0.388]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.284 -0.029  0.732 ...  0.174  0.204 -0.257]\n",
            "  [ 0.307  0.14   0.724 ...  0.218  0.184 -0.258]\n",
            "  [ 0.289  0.052  0.657 ...  0.228  0.188 -0.259]\n",
            "  [ 0.285  0.063  0.677 ...  0.225  0.191 -0.263]\n",
            "  [ 0.278  0.058  0.676 ...  0.231  0.197 -0.262]]\n",
            "\n",
            " [[-0.451 -0.13  -2.606 ... -0.903 -0.666 -0.455]\n",
            "  [-0.307 -0.048 -2.646 ... -1.21  -0.83  -0.511]\n",
            "  [-0.235  0.056 -2.658 ... -1.29  -0.853 -0.549]\n",
            "  [-0.194  0.053 -2.682 ... -1.343 -0.876 -0.569]\n",
            "  [-0.174  0.063 -2.684 ... -1.389 -0.888 -0.583]]\n",
            "\n",
            " [[ 0.086  0.809 -0.729 ... -0.58   0.936  4.029]\n",
            "  [-0.503  0.514 -0.735 ... -0.337  1.5    4.095]\n",
            "  [-0.409  0.431 -0.813 ... -0.432  2.059  4.152]\n",
            "  [-0.413  0.479 -0.774 ... -0.488  2.179  4.099]\n",
            "  [-0.391  0.44  -0.798 ... -0.507  2.378  4.099]]]\n",
            "---Actual---\n",
            "[[[ 0.327 -0.37  -0.411 ... -1.33  -0.748 -0.339]\n",
            "  [ 0.372 -0.35  -0.434 ... -1.337 -0.764 -0.345]\n",
            "  [ 0.327 -0.37  -0.434 ... -1.337 -0.764 -0.349]\n",
            "  [ 0.38  -0.37  -0.434 ... -1.334 -0.764 -0.359]\n",
            "  [ 0.323 -0.389 -0.434 ... -1.334 -0.764 -0.366]]\n",
            "\n",
            " [[ 0.128  0.597  0.466 ...  1.407  0.837 -0.149]\n",
            "  [ 0.145  0.597  0.443 ...  1.352  0.966 -0.158]\n",
            "  [ 0.128  0.597  0.443 ...  1.315  1.031 -0.158]\n",
            "  [ 0.145  0.597  0.443 ...  1.3    1.079 -0.156]\n",
            "  [ 0.124  0.597  0.443 ...  1.275  1.112 -0.149]]\n",
            "\n",
            " [[-1.566  1.06   0.143 ... -0.969 -0.538 -0.327]\n",
            "  [-1.558  1.041  0.097 ... -0.969 -0.522 -0.333]\n",
            "  [-1.554  1.002  0.12  ... -0.955 -0.522 -0.339]\n",
            "  [-1.558  0.983  0.143 ... -0.955 -0.522 -0.339]\n",
            "  [-1.558  0.964  0.097 ... -0.947 -0.522 -0.339]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.136 -0.06   0.928 ...  0.005  0.934 -0.279]\n",
            "  [ 0.145 -0.06   0.928 ... -0.017  0.885 -0.288]\n",
            "  [ 0.124 -0.06   0.905 ... -0.024  0.804 -0.294]\n",
            "  [ 0.149 -0.08   0.905 ... -0.006  0.723 -0.284]\n",
            "  [ 0.12  -0.08   0.951 ...  0.005  0.675 -0.277]]\n",
            "\n",
            " [[-0.391  0.171 -2.649 ... -1.183 -0.7   -0.403]\n",
            "  [-0.33   0.191 -2.649 ... -1.19  -0.7   -0.407]\n",
            "  [-0.346  0.191 -2.672 ... -1.205 -0.7   -0.411]\n",
            "  [-0.391  0.191 -2.672 ... -1.22  -0.7   -0.41 ]\n",
            "  [-0.33   0.191 -2.649 ... -1.231 -0.7   -0.413]]\n",
            "\n",
            " [[-0.5    0.229 -1.057 ... -0.179  1.37   3.763]\n",
            "  [-0.48   0.21  -1.057 ... -0.16   1.516  3.763]\n",
            "  [-0.468  0.171 -1.057 ... -0.16   1.564  3.763]\n",
            "  [-0.5    0.191 -1.057 ... -0.171  1.516  3.763]\n",
            "  [-0.484  0.191 -1.057 ... -0.19   1.5    3.763]]]\n"
          ]
        }
      ]
    }
  ]
}