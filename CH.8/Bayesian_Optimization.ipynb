{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "mount_file_id": "1_SNGxrHuPGtuEV0E3L7XJL2OMCf6MeWq",
      "authorship_tag": "ABX9TyOgMNer8hgbqRWZU2QFv8BE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simseoyoung/Deep-Learning/blob/main/CH.8/Bayesian_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Optimization\n",
        "\n",
        "- learning rate과 weight decay를 bayesian optimization함\n",
        "\n",
        "- lstm 사용\n",
        "\n",
        "- dataset : Amazon 주식 dataset (AMZN.csv)"
      ],
      "metadata": {
        "id": "STr8j_T6RC5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B--gPqKRCUH",
        "outputId": "6d7eba75-388e-4984-b6dd-dd9f941a2234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVZDrHKbUKY_",
        "outputId": "d9f7fbff-427b-44dc-ff6d-4db2eabcc0f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Data processing"
      ],
      "metadata": {
        "id": "VlpnZgxnUX36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 불러오기\n",
        "train_data_url = \"/content/drive/MyDrive/DL 실습/AMZN.csv\"\n",
        "df = pd.read_csv(train_data_url)\n",
        "print (df.shape) # 파일 형식 확인\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "pwnTbRUYUWIf",
        "outputId": "9ad08145-6a7e-47d9-fd56-dbcc198e5116"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5178, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date         Open         High          Low        Close  \\\n",
              "0     1997-05-15     2.437500     2.500000     1.927083     1.958333   \n",
              "1     1997-05-16     1.968750     1.979167     1.708333     1.729167   \n",
              "2     1997-05-19     1.760417     1.770833     1.625000     1.708333   \n",
              "3     1997-05-20     1.729167     1.750000     1.635417     1.635417   \n",
              "4     1997-05-21     1.635417     1.645833     1.375000     1.427083   \n",
              "...          ...          ...          ...          ...          ...   \n",
              "5173  2017-12-04  1173.849976  1175.199951  1128.000000  1133.949951   \n",
              "5174  2017-12-05  1128.260010  1159.270020  1124.739990  1141.569946   \n",
              "5175  2017-12-06  1137.989990  1155.890015  1136.079956  1152.349976   \n",
              "5176  2017-12-07  1156.589966  1163.189941  1151.000000  1159.790039   \n",
              "5177  2017-12-08  1170.400024  1172.750000  1157.099976  1162.000000   \n",
              "\n",
              "        Adj Close    Volume  \n",
              "0        1.958333  72156000  \n",
              "1        1.729167  14700000  \n",
              "2        1.708333   6106800  \n",
              "3        1.635417   5467200  \n",
              "4        1.427083  18853200  \n",
              "...           ...       ...  \n",
              "5173  1133.949951   5931900  \n",
              "5174  1141.569946   4079800  \n",
              "5175  1152.349976   2853300  \n",
              "5176  1159.790039   2508800  \n",
              "5177  1162.000000   2711530  \n",
              "\n",
              "[5178 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02d5749c-d31b-4e7c-ae46-43bd3fbbfe09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1997-05-15</td>\n",
              "      <td>2.437500</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.927083</td>\n",
              "      <td>1.958333</td>\n",
              "      <td>1.958333</td>\n",
              "      <td>72156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1997-05-16</td>\n",
              "      <td>1.968750</td>\n",
              "      <td>1.979167</td>\n",
              "      <td>1.708333</td>\n",
              "      <td>1.729167</td>\n",
              "      <td>1.729167</td>\n",
              "      <td>14700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1997-05-19</td>\n",
              "      <td>1.760417</td>\n",
              "      <td>1.770833</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>1.708333</td>\n",
              "      <td>1.708333</td>\n",
              "      <td>6106800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1997-05-20</td>\n",
              "      <td>1.729167</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>1.635417</td>\n",
              "      <td>1.635417</td>\n",
              "      <td>1.635417</td>\n",
              "      <td>5467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1997-05-21</td>\n",
              "      <td>1.635417</td>\n",
              "      <td>1.645833</td>\n",
              "      <td>1.375000</td>\n",
              "      <td>1.427083</td>\n",
              "      <td>1.427083</td>\n",
              "      <td>18853200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5173</th>\n",
              "      <td>2017-12-04</td>\n",
              "      <td>1173.849976</td>\n",
              "      <td>1175.199951</td>\n",
              "      <td>1128.000000</td>\n",
              "      <td>1133.949951</td>\n",
              "      <td>1133.949951</td>\n",
              "      <td>5931900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5174</th>\n",
              "      <td>2017-12-05</td>\n",
              "      <td>1128.260010</td>\n",
              "      <td>1159.270020</td>\n",
              "      <td>1124.739990</td>\n",
              "      <td>1141.569946</td>\n",
              "      <td>1141.569946</td>\n",
              "      <td>4079800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5175</th>\n",
              "      <td>2017-12-06</td>\n",
              "      <td>1137.989990</td>\n",
              "      <td>1155.890015</td>\n",
              "      <td>1136.079956</td>\n",
              "      <td>1152.349976</td>\n",
              "      <td>1152.349976</td>\n",
              "      <td>2853300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5176</th>\n",
              "      <td>2017-12-07</td>\n",
              "      <td>1156.589966</td>\n",
              "      <td>1163.189941</td>\n",
              "      <td>1151.000000</td>\n",
              "      <td>1159.790039</td>\n",
              "      <td>1159.790039</td>\n",
              "      <td>2508800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5177</th>\n",
              "      <td>2017-12-08</td>\n",
              "      <td>1170.400024</td>\n",
              "      <td>1172.750000</td>\n",
              "      <td>1157.099976</td>\n",
              "      <td>1162.000000</td>\n",
              "      <td>1162.000000</td>\n",
              "      <td>2711530</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5178 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02d5749c-d31b-4e7c-ae46-43bd3fbbfe09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-02d5749c-d31b-4e7c-ae46-43bd3fbbfe09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-02d5749c-d31b-4e7c-ae46-43bd3fbbfe09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요없는 항목(Volume : 거래량) 제거\n",
        "X = df.drop(columns = ['Volume','Date'])\n",
        "# y값은 Adj Close 로 지정 ( 종가 )\n",
        "y = df.iloc[:, 5:6]\n",
        "\n",
        "print (X)\n",
        "print (y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNdjuoF_UpZZ",
        "outputId": "744b528b-1d5d-4c45-e0ae-2dd8154561a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Open         High          Low        Close    Adj Close\n",
            "0        2.437500     2.500000     1.927083     1.958333     1.958333\n",
            "1        1.968750     1.979167     1.708333     1.729167     1.729167\n",
            "2        1.760417     1.770833     1.625000     1.708333     1.708333\n",
            "3        1.729167     1.750000     1.635417     1.635417     1.635417\n",
            "4        1.635417     1.645833     1.375000     1.427083     1.427083\n",
            "...           ...          ...          ...          ...          ...\n",
            "5173  1173.849976  1175.199951  1128.000000  1133.949951  1133.949951\n",
            "5174  1128.260010  1159.270020  1124.739990  1141.569946  1141.569946\n",
            "5175  1137.989990  1155.890015  1136.079956  1152.349976  1152.349976\n",
            "5176  1156.589966  1163.189941  1151.000000  1159.790039  1159.790039\n",
            "5177  1170.400024  1172.750000  1157.099976  1162.000000  1162.000000\n",
            "\n",
            "[5178 rows x 5 columns]\n",
            "        Adj Close\n",
            "0        1.958333\n",
            "1        1.729167\n",
            "2        1.708333\n",
            "3        1.635417\n",
            "4        1.427083\n",
            "...           ...\n",
            "5173  1133.949951\n",
            "5174  1141.569946\n",
            "5175  1152.349976\n",
            "5176  1159.790039\n",
            "5177  1162.000000\n",
            "\n",
            "[5178 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "scaler.fit(y)\n",
        "X_scale = scaler.fit_transform(X)\n",
        "y_scale = scaler.fit_transform(y)\n",
        "\n",
        "# train, test data set 나누기\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scale, y_scale, test_size = 0.3, shuffle = True, random_state = 1234)\n",
        "\n",
        "# train set\n",
        "print(\"x_train의 크기: \",x_train.shape)\n",
        "print(\"y_train의 크기: \",y_train.shape)\n",
        "\n",
        "# test set\n",
        "print(\"x_test의 크기: \",x_test.shape)\n",
        "print(\"y_test의 크기: \",y_test.shape)\n",
        "\n",
        "# data 텐서로 변환\n",
        "x_train_tensor = torch.FloatTensor(x_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "x_test_tensor = torch.FloatTensor(x_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "X_train_tensors_final = torch.reshape(x_train_tensor,   \n",
        "                        (x_train_tensor.shape[0], 1, x_train_tensor.shape[1]))\n",
        "X_test_tensors_final = torch.reshape(x_test_tensor,  \n",
        "                        (x_test_tensor.shape[0], 1, x_test_tensor.shape[1])) \n",
        "\n",
        "# train set\n",
        "print(\"x_train_shape: \",x_train_tensor.shape)\n",
        "print(\"y_train_shape:: \",y_train_tensor.shape)\n",
        "\n",
        "# test set\n",
        "print(\"x_test_shape: \",x_test_tensor.shape)\n",
        "print(\"y_test_shape: \",y_test_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZQpwt9oUsEQ",
        "outputId": "93f26d61-ce0f-4839-a5e1-15556dfd7b53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train의 크기:  (3624, 5)\n",
            "y_train의 크기:  (3624, 1)\n",
            "x_test의 크기:  (1554, 5)\n",
            "y_test의 크기:  (1554, 1)\n",
            "x_train_shape:  torch.Size([3624, 5])\n",
            "y_train_shape::  torch.Size([3624, 1])\n",
            "x_test_shape:  torch.Size([1554, 5])\n",
            "y_test_shape:  torch.Size([1554, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Model"
      ],
      "metadata": {
        "id": "EC89s5gOU0QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_layers = num_layers\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                         num_layers = num_layers, batch_first = True)\n",
        "    self.layer1 = nn.Linear(hidden_size, 256)\n",
        "    self.layer2 = nn.Linear(256, 256)\n",
        "    self.layer3 = nn.Linear(256, 128)\n",
        "    self.layer_out = nn.Linear(128, num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # hidden state\n",
        "    h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    # internal process state\n",
        "    c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
        "\n",
        "    # Data reshaping\n",
        "    hn = hn.view(-1, self.hidden_size)\n",
        "\n",
        "    out = self.relu(hn)\n",
        "    out = self.layer1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer_out(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "v2FMRd4vUzRh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set parameter\n",
        "\n",
        "num_epoch = 50\n",
        "learning_rate = 0.01\n",
        "weight_decay = 1e-5\n",
        "\n",
        "input_size = 5\n",
        "hidden_size = 2\n",
        "num_layers = 1\n",
        "num_classes = 1\n",
        "\n",
        "lstm = LSTM(num_classes, input_size, hidden_size, num_layers, x_train_tensor.shape[1])\n",
        "loss_function = torch.nn.MSELoss() \n"
      ],
      "metadata": {
        "id": "z9o1vrXiVloV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_acc(lr, wd):\n",
        "\n",
        "    loss_list = []\n",
        "    learning_rate = lr\n",
        "    weight_decay = wd\n",
        "    \n",
        "    optimizer = optim.Adam(lstm.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "    for epoch in range(num_epoch):\n",
        "      outputs = lstm.forward(X_train_tensors_final.to(device))\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss = loss_function(outputs, y_train_tensor.to(device))\n",
        "      loss_list.append(loss.detach().numpy())\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    return loss.detach().numpy()\n",
        "# loss를 기준으로 hyperparameter 찾도록 함"
      ],
      "metadata": {
        "id": "APRuQXmnWGbL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "pbounds = {'lr':(0,1),  'wd':(0,1)} # 하이퍼파라미터 범위 설정\n",
        "optimizer = BayesianOptimization( f=train_acc, pbounds= pbounds, random_state=1 )\n",
        "\n",
        "optimizer.maximize( init_points= 5, n_iter=45 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2AjkgVRU_Zf",
        "outputId": "1151ab5e-c664-4b90-c1b7-e7c72e07cb96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |    lr     |    wd     |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 120.6   \u001b[0m | \u001b[0m 0.417   \u001b[0m | \u001b[0m 0.7203  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.539   \u001b[0m | \u001b[0m 0.000114\u001b[0m | \u001b[0m 0.3023  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5924  \u001b[0m | \u001b[0m 0.1468  \u001b[0m | \u001b[0m 0.09234 \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3706  \u001b[0m | \u001b[0m 0.1863  \u001b[0m | \u001b[0m 0.3456  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 7.08    \u001b[0m | \u001b[0m 0.3968  \u001b[0m | \u001b[0m 0.5388  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5655  \u001b[0m | \u001b[0m 0.1088  \u001b[0m | \u001b[0m 0.3689  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 1.654   \u001b[0m | \u001b[0m 0.4295  \u001b[0m | \u001b[0m 0.8212  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 5.841   \u001b[0m | \u001b[0m 0.1493  \u001b[0m | \u001b[0m 0.6288  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 12.86   \u001b[0m | \u001b[0m 0.4152  \u001b[0m | \u001b[0m 0.7072  \u001b[0m |\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m 315.2   \u001b[0m | \u001b[95m 0.3956  \u001b[0m | \u001b[95m 0.1615  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 87.31   \u001b[0m | \u001b[0m 0.8877  \u001b[0m | \u001b[0m 0.4472  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 15.22   \u001b[0m | \u001b[0m 0.4323  \u001b[0m | \u001b[0m 0.352   \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 18.74   \u001b[0m | \u001b[0m 0.6099  \u001b[0m | \u001b[0m 0.3929  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 14.56   \u001b[0m | \u001b[0m 0.5532  \u001b[0m | \u001b[0m 0.702   \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m 4.917   \u001b[0m | \u001b[0m 0.1484  \u001b[0m | \u001b[0m 0.4067  \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.2254  \u001b[0m | \u001b[0m 0.06681 \u001b[0m | \u001b[0m 0.5343  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m 33.36   \u001b[0m | \u001b[0m 0.3392  \u001b[0m | \u001b[0m 0.2749  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.2067  \u001b[0m | \u001b[0m 0.1909  \u001b[0m | \u001b[0m 0.6735  \u001b[0m |\n",
            "| \u001b[95m 19      \u001b[0m | \u001b[95m 3.341e+0\u001b[0m | \u001b[95m 0.9098  \u001b[0m | \u001b[95m 0.4161  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m 3.765e+0\u001b[0m | \u001b[0m 0.1681  \u001b[0m | \u001b[0m 0.6841  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m 2.24    \u001b[0m | \u001b[0m 0.9201  \u001b[0m | \u001b[0m 0.4042  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m 15.72   \u001b[0m | \u001b[0m 0.07974 \u001b[0m | \u001b[0m 0.6272  \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.1863  \u001b[0m | \u001b[0m 0.01099 \u001b[0m | \u001b[0m 0.3749  \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m 1.336   \u001b[0m | \u001b[0m 0.2195  \u001b[0m | \u001b[0m 0.8328  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m 1.277   \u001b[0m | \u001b[0m 0.5103  \u001b[0m | \u001b[0m 0.7786  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m 2.793   \u001b[0m | \u001b[0m 0.8441  \u001b[0m | \u001b[0m 0.6097  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m 299.1   \u001b[0m | \u001b[0m 0.7915  \u001b[0m | \u001b[0m 0.7686  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 0.01034 \u001b[0m | \u001b[0m 0.365   \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m 78.45   \u001b[0m | \u001b[0m 0.4614  \u001b[0m | \u001b[0m 0.488   \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m 1.125e+0\u001b[0m | \u001b[0m 0.9029  \u001b[0m | \u001b[0m 0.4189  \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m 6.427   \u001b[0m | \u001b[0m 0.3143  \u001b[0m | \u001b[0m 0.2145  \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m 3.85e+03\u001b[0m | \u001b[0m 0.9083  \u001b[0m | \u001b[0m 0.4133  \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m 5.867e+0\u001b[0m | \u001b[0m 0.9046  \u001b[0m | \u001b[0m 0.4237  \u001b[0m |\n",
            "| \u001b[95m 34      \u001b[0m | \u001b[95m 2.821e+0\u001b[0m | \u001b[95m 0.9111  \u001b[0m | \u001b[95m 0.4176  \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m 3.502e+0\u001b[0m | \u001b[0m 0.7387  \u001b[0m | \u001b[0m 0.2981  \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m 9.083e+0\u001b[0m | \u001b[0m 0.6659  \u001b[0m | \u001b[0m 0.9458  \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m 1.331   \u001b[0m | \u001b[0m 0.2224  \u001b[0m | \u001b[0m 0.8813  \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.4698  \u001b[0m | \u001b[0m 0.08495 \u001b[0m | \u001b[0m 0.6477  \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m 2.849e+0\u001b[0m | \u001b[0m 0.8952  \u001b[0m | \u001b[0m 0.4704  \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m 3.703e+0\u001b[0m | \u001b[0m 0.4322  \u001b[0m | \u001b[0m 0.8655  \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m 2.569   \u001b[0m | \u001b[0m 0.0279  \u001b[0m | \u001b[0m 0.1382  \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m 737.8   \u001b[0m | \u001b[0m 0.926   \u001b[0m | \u001b[0m 0.4566  \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m 2.328   \u001b[0m | \u001b[0m 0.8823  \u001b[0m | \u001b[0m 0.8403  \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m 2.009e+0\u001b[0m | \u001b[0m 0.6112  \u001b[0m | \u001b[0m 0.809   \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m 13.32   \u001b[0m | \u001b[0m 0.06263 \u001b[0m | \u001b[0m 0.5274  \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.4737  \u001b[0m | \u001b[0m 0.3433  \u001b[0m | \u001b[0m 0.6393  \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m 130.3   \u001b[0m | \u001b[0m 0.7272  \u001b[0m | \u001b[0m 0.2415  \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m 5.246   \u001b[0m | \u001b[0m 0.7515  \u001b[0m | \u001b[0m 0.8588  \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m 1.145   \u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 0.6697  \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m 2.461   \u001b[0m | \u001b[0m 0.2926  \u001b[0m | \u001b[0m 0.9978  \u001b[0m |\n",
            "=================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( optimizer.max )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfFevS93Y3RP",
        "outputId": "ba780f01-4677-45ba-ea2f-a9133009258b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': 2820995.5, 'params': {'lr': 0.9111313637898341, 'wd': 0.41757886369625574}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparmeter로 학습\n",
        "learning_rate =  0.9111313637898341\n",
        "weight_decay =  0.41757886369625574\n",
        "num_epoch = 1000\n",
        "lstm = LSTM(num_classes, input_size, hidden_size, num_layers, x_train_tensor.shape[1])\n",
        "loss_function = torch.nn.MSELoss() \n",
        "optimizer = optim.Adam(lstm.parameters(), lr = learning_rate)\n",
        "loss_list = []\n",
        "for epoch in range(num_epoch):\n",
        "  outputs = lstm.forward(X_train_tensors_final.to(device))\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss = loss_function(outputs, y_train_tensor.to(device))\n",
        "  loss_list.append(loss.detach().numpy())\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  if epoch%100 == 0:\n",
        "    print('Epoch: {}, loss: {}%'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOtYpIleZMVJ",
        "outputId": "075c2c5c-2d59-4369-a43c-4e82f9eff012"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.9706291556358337%\n",
            "Epoch: 100, loss: 0.9895516037940979%\n",
            "Epoch: 200, loss: 0.7880679368972778%\n",
            "Epoch: 300, loss: 0.6565371751785278%\n",
            "Epoch: 400, loss: 0.5529198050498962%\n",
            "Epoch: 500, loss: 0.4656383991241455%\n",
            "Epoch: 600, loss: 0.4072442948818207%\n",
            "Epoch: 700, loss: 0.36380234360694885%\n",
            "Epoch: 800, loss: 0.36309295892715454%\n",
            "Epoch: 900, loss: 0.34587180614471436%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize = (10,6)) \n",
        "plt.plot(loss_list, label = 'loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss')\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "K44tZlhYZhr3",
        "outputId": "d9834164-14ed-4c28-ab6e-d90db3525c3f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}