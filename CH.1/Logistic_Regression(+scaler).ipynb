{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD5f60HX0bMeMx7nyU/9ph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simseoyoung/Deep-Learning/blob/main/CH.1/Logistic_Regression(%2Bscaler).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "- Data set : sklearn이 제공하는 IRIS\n",
        "\n",
        "  -> 꽃의 종류와 꽃의 특징 4가지로 이루어져 있음\n",
        "\n",
        "- Scaler 종류 바꿔가며 비교해보기"
      ],
      "metadata": {
        "id": "qJ44nwew9ZWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zPwjmctD9Vf-"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Data Processing"
      ],
      "metadata": {
        "id": "K1gVysDd9YeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_iris()\n",
        "\n",
        "data\n",
        "# dictionary 형태로 저장되어 있으므로 CSV 형태로 바꿔줘야 함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Odc-YhG951m",
        "outputId": "82d3829a-3607-475e-8a48-b689549541df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'data_module': 'sklearn.datasets.data',\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': 'iris.csv',\n",
              " 'frame': None,\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = data['data']\n",
        "target_data = data['target']\n",
        "\n",
        "name = data['target_names']\n",
        "feature = data['feature_names']\n",
        "\n",
        "# Dataframe 사용하여 정리\n",
        "iris_df = pd.DataFrame(input_data, columns=feature)\n",
        "iris_df['species'] = target_data\n",
        "\n",
        "print(iris_df.shape)\n",
        "iris_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "s1_vkOHw_rH_",
        "outputId": "f2365b6a-1f65-42f1-af0d-4b08a6708675"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                  5.1               3.5                1.4               0.2   \n",
              "1                  4.9               3.0                1.4               0.2   \n",
              "2                  4.7               3.2                1.3               0.2   \n",
              "3                  4.6               3.1                1.5               0.2   \n",
              "4                  5.0               3.6                1.4               0.2   \n",
              "..                 ...               ...                ...               ...   \n",
              "145                6.7               3.0                5.2               2.3   \n",
              "146                6.3               2.5                5.0               1.9   \n",
              "147                6.5               3.0                5.2               2.0   \n",
              "148                6.2               3.4                5.4               2.3   \n",
              "149                5.9               3.0                5.1               1.8   \n",
              "\n",
              "     species  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "..       ...  \n",
              "145        2  \n",
              "146        2  \n",
              "147        2  \n",
              "148        2  \n",
              "149        2  \n",
              "\n",
              "[150 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f473a6cc-6f04-491d-ae99-f6fb19a07326\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f473a6cc-6f04-491d-ae99-f6fb19a07326')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f473a6cc-6f04-491d-ae99-f6fb19a07326 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f473a6cc-6f04-491d-ae99-f6fb19a07326');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test data 나누기\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_data, target_data, test_size = 0.2, random_state = 1234 )\n",
        "print(\"x_train의 크기:{}\",x_train.shape)\n",
        "print(\"y_train의 크기:{}\",y_train.shape)\n",
        "print(\"x_testn의 크기:{}\",x_test.shape)\n",
        "print(\"y_test의 크기:{}\",y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnE6s2snAB21",
        "outputId": "a87b8f70-328e-4573-de40-a061a4ee9bd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train의 크기:{} (120, 4)\n",
            "y_train의 크기:{} (120,)\n",
            "x_testn의 크기:{} (30, 4)\n",
            "y_test의 크기:{} (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 데이터 Scaling\n",
        "#scaler = StandardScaler()\n",
        "#scaler = MinMaxScaler()\n",
        "scaler = RobustScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train_tran = scaler.transform(x_train)\n",
        "\n",
        "\n",
        "#학습 데이터 배치화 시키기 \n",
        "train_data = data_utils.TensorDataset(torch.FloatTensor(x_train_tran),\n",
        "                                 torch.FloatTensor(y_train))\n",
        "\n",
        "dataloader = data_utils.DataLoader(train_data, batch_size=40, shuffle=False)\n",
        "\n",
        "\n",
        "#배치화된 데이터 확인\n",
        "for batch_idx, datas in enumerate(dataloader):\n",
        "    print(batch_idx)\n",
        "    print(datas[0].shape)\n",
        "    print(datas[1].shape)\n",
        "    break\n",
        "\n",
        "    \n",
        "#테스트 데이터 Scaling    \n",
        "x_test_tran = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "#테스트 데이터 텐서로 변환\n",
        "x_test_tensor = torch.FloatTensor(x_test_tran)\n",
        "y_test_tensor = torch.FloatTensor(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wBzaWweA0us",
        "outputId": "41a4196a-69d1-494f-8a40-98a4028aa549"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([40, 4])\n",
            "torch.Size([40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Model"
      ],
      "metadata": {
        "id": "F8yEVWPXBmnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "        def __init__(self, input_size, output_size):\n",
        "            super(LogisticRegression, self).__init__()\n",
        "            self.input_size = input_size\n",
        "            self.output_size = output_size\n",
        "            self.linear = torch.nn.Linear(self.input_size, self.output_size)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "            \n",
        "        def forward(self, input_tensor):\n",
        "            linear1 = self.linear(input_tensor)\n",
        "            output = self.sigmoid(linear1)\n",
        "            return output\n",
        "\n",
        "#Hyper-parameter\n",
        "input_size = 4\n",
        "output_size = 1\n",
        "learning_rate = 0.1\n",
        "n_epochs = 200\n",
        "\n",
        "#model 생성\n",
        "model = LogisticRegression(input_size=input_size, output_size=output_size)\n",
        "\n",
        "#손실함수 생성\n",
        "criterion = torch.nn.MSELoss() \n",
        "#Optimizer 생성\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "ypJBRA5OBmU9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3] Training"
      ],
      "metadata": {
        "id": "UW8HpBzKB25S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(n_epochs+1):\n",
        "    for idx, (x_batch, y_batch) in enumerate(dataloader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x_batch)\n",
        "        y_pred = y_pred.reshape(-1)\n",
        "\n",
        "        loss_train = criterion(y_pred.squeeze(), y_batch)\n",
        "\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Validation\n",
        "        model.eval()\n",
        "        y_test_pred = model(x_test_tensor)\n",
        "        \n",
        "        y_test_pred = y_test_pred.reshape(-1)\n",
        "        \n",
        "        loss_test = criterion(y_test_pred, y_test_tensor)\n",
        "    train_loss.append(loss_train.item())\n",
        "    test_loss.append(loss_test.item())\n",
        "    print(\"epoch:{}, Loss_train:{:.2f}, Loss_test:{:.2f}\".format( epoch, train_loss[-1], test_loss[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lbB7y1NCGap",
        "outputId": "c386fe19-d973-4d7e-fe99-042e55df9aa2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, Loss_train:1.05, Loss_test:0.78\n",
            "epoch:1, Loss_train:1.00, Loss_test:0.73\n",
            "epoch:2, Loss_train:0.94, Loss_test:0.69\n",
            "epoch:3, Loss_train:0.89, Loss_test:0.65\n",
            "epoch:4, Loss_train:0.85, Loss_test:0.61\n",
            "epoch:5, Loss_train:0.81, Loss_test:0.58\n",
            "epoch:6, Loss_train:0.78, Loss_test:0.55\n",
            "epoch:7, Loss_train:0.75, Loss_test:0.53\n",
            "epoch:8, Loss_train:0.72, Loss_test:0.51\n",
            "epoch:9, Loss_train:0.70, Loss_test:0.49\n",
            "epoch:10, Loss_train:0.68, Loss_test:0.47\n",
            "epoch:11, Loss_train:0.66, Loss_test:0.46\n",
            "epoch:12, Loss_train:0.64, Loss_test:0.45\n",
            "epoch:13, Loss_train:0.63, Loss_test:0.43\n",
            "epoch:14, Loss_train:0.62, Loss_test:0.42\n",
            "epoch:15, Loss_train:0.60, Loss_test:0.42\n",
            "epoch:16, Loss_train:0.59, Loss_test:0.41\n",
            "epoch:17, Loss_train:0.58, Loss_test:0.40\n",
            "epoch:18, Loss_train:0.58, Loss_test:0.39\n",
            "epoch:19, Loss_train:0.57, Loss_test:0.39\n",
            "epoch:20, Loss_train:0.56, Loss_test:0.38\n",
            "epoch:21, Loss_train:0.55, Loss_test:0.38\n",
            "epoch:22, Loss_train:0.55, Loss_test:0.37\n",
            "epoch:23, Loss_train:0.54, Loss_test:0.37\n",
            "epoch:24, Loss_train:0.54, Loss_test:0.36\n",
            "epoch:25, Loss_train:0.53, Loss_test:0.36\n",
            "epoch:26, Loss_train:0.53, Loss_test:0.36\n",
            "epoch:27, Loss_train:0.52, Loss_test:0.35\n",
            "epoch:28, Loss_train:0.52, Loss_test:0.35\n",
            "epoch:29, Loss_train:0.51, Loss_test:0.35\n",
            "epoch:30, Loss_train:0.51, Loss_test:0.34\n",
            "epoch:31, Loss_train:0.51, Loss_test:0.34\n",
            "epoch:32, Loss_train:0.50, Loss_test:0.34\n",
            "epoch:33, Loss_train:0.50, Loss_test:0.34\n",
            "epoch:34, Loss_train:0.50, Loss_test:0.33\n",
            "epoch:35, Loss_train:0.50, Loss_test:0.33\n",
            "epoch:36, Loss_train:0.49, Loss_test:0.33\n",
            "epoch:37, Loss_train:0.49, Loss_test:0.33\n",
            "epoch:38, Loss_train:0.49, Loss_test:0.33\n",
            "epoch:39, Loss_train:0.49, Loss_test:0.33\n",
            "epoch:40, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:41, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:42, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:43, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:44, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:45, Loss_train:0.48, Loss_test:0.32\n",
            "epoch:46, Loss_train:0.47, Loss_test:0.32\n",
            "epoch:47, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:48, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:49, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:50, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:51, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:52, Loss_train:0.47, Loss_test:0.31\n",
            "epoch:53, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:54, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:55, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:56, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:57, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:58, Loss_train:0.46, Loss_test:0.31\n",
            "epoch:59, Loss_train:0.46, Loss_test:0.30\n",
            "epoch:60, Loss_train:0.46, Loss_test:0.30\n",
            "epoch:61, Loss_train:0.46, Loss_test:0.30\n",
            "epoch:62, Loss_train:0.46, Loss_test:0.30\n",
            "epoch:63, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:64, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:65, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:66, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:67, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:68, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:69, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:70, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:71, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:72, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:73, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:74, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:75, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:76, Loss_train:0.45, Loss_test:0.30\n",
            "epoch:77, Loss_train:0.45, Loss_test:0.29\n",
            "epoch:78, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:79, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:80, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:81, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:82, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:83, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:84, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:85, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:86, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:87, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:88, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:89, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:90, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:91, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:92, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:93, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:94, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:95, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:96, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:97, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:98, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:99, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:100, Loss_train:0.44, Loss_test:0.29\n",
            "epoch:101, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:102, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:103, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:104, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:105, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:106, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:107, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:108, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:109, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:110, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:111, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:112, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:113, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:114, Loss_train:0.43, Loss_test:0.29\n",
            "epoch:115, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:116, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:117, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:118, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:119, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:120, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:121, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:122, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:123, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:124, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:125, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:126, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:127, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:128, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:129, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:130, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:131, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:132, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:133, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:134, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:135, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:136, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:137, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:138, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:139, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:140, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:141, Loss_train:0.43, Loss_test:0.28\n",
            "epoch:142, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:143, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:144, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:145, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:146, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:147, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:148, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:149, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:150, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:151, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:152, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:153, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:154, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:155, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:156, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:157, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:158, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:159, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:160, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:161, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:162, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:163, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:164, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:165, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:166, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:167, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:168, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:169, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:170, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:171, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:172, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:173, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:174, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:175, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:176, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:177, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:178, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:179, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:180, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:181, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:182, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:183, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:184, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:185, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:186, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:187, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:188, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:189, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:190, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:191, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:192, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:193, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:194, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:195, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:196, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:197, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:198, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:199, Loss_train:0.42, Loss_test:0.28\n",
            "epoch:200, Loss_train:0.42, Loss_test:0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss 값 plot\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, label='train')\n",
        "plt.plot(test_loss, label='test')\n",
        "plt.title('Model loss')\n",
        "plt.legend(loc= 'upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "lSseaaClDDB1",
        "outputId": "099ab90f-f631-4032-8018-47f1fa3389d9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}