{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simseoyoung/Deep-Learning/blob/main/CH.2/Chap2_1)_Regression_with_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1jYfAmkwCC0"
      },
      "source": [
        "# MLP(Multi Layer Perceptron) - Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzjsHwu3wCC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdWNWV6LwCDA"
      },
      "source": [
        "### 1) Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "fVOGI0MBwCDB",
        "outputId": "ed28a602-c3f8-4dd6-e551-55767897b633"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7715f3fc98e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#데이터는 CH1 regression에서 사용했던 파일과 동일\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BostonHousing_(1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BostonHousing_(1).csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#csv(comma-separated values)파일 가져오기 (디렉토리 주소입력)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: upload() takes 0 positional arguments but 1 was given"
          ]
        }
      ],
      "source": [
        "#데이터 판다스로 불러오기\n",
        "#데이터는 CH1 regression에서 사용했던 파일과 동일\n",
        "\n",
        "df_raw = pd.read_csv('./BostonHousing_(1).csv') #csv(comma-separated values)파일 가져오기 (디렉토리 주소입력)\n",
        "print(type(df_raw))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_9TQRR22JsT",
        "outputId": "65aa6ab5-a05f-4ced-eca9-88cae90dde45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gutLXPT0wCDE",
        "outputId": "2b6874b3-3e1c-4871-c876-9b8df3da7d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(506, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>black</th>\n",
              "      <th>lstat</th>\n",
              "      <th>medv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
              "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
              "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
              "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
              "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
              "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
              "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
              "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
              "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
              "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
              "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
              "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
              "\n",
              "     ptratio   black  lstat  medv  \n",
              "0       15.3  396.90   4.98  24.0  \n",
              "1       17.8  396.90   9.14  21.6  \n",
              "2       17.8  392.83   4.03  34.7  \n",
              "3       18.7  394.63   2.94  33.4  \n",
              "4       18.7  396.90   5.33  36.2  \n",
              "..       ...     ...    ...   ...  \n",
              "501     21.0  391.99   9.67  22.4  \n",
              "502     21.0  396.90   9.08  20.6  \n",
              "503     21.0  396.90   5.64  23.9  \n",
              "504     21.0  393.45   6.48  22.0  \n",
              "505     21.0  396.90   7.88  11.9  \n",
              "\n",
              "[506 rows x 14 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_raw.shape) # Dataset의 크기 확인\n",
        "df_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "mQvRjXkJwCDF",
        "outputId": "03736c5f-89b0-40ef-835e-478bc4d75c96"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-884656852fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X값 coulumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"medv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Target 값에 해당하는 집값('medv') 열을 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y값 column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"medv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
          ]
        }
      ],
      "source": [
        "# X값 coulumn\n",
        "x = df_raw.drop([\"medv\"], axis=1) #Target 값에 해당하는 집값('medv') 열을 삭제\n",
        "\n",
        "# y값 column\n",
        "y = df_raw[\"medv\"]\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(type(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Gd2CS2wCDH",
        "outputId": "c1cd8bc6-2280-454f-8ca9-96ee668060ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train의 크기:  (354, 13)\n",
            "y_train의 크기:  (354,) \n",
            "\n",
            "x_test의 크기:  (152, 13)\n",
            "y_test의 크기:  (152,)\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "#학습데이터와 테스트데이터를 일정비율로 나누기\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state=1234)\n",
        "\n",
        "#학습 데이터\n",
        "print(\"x_train의 크기: \",x_train.shape)\n",
        "print(\"y_train의 크기: \",y_train.shape,'\\n')\n",
        "\n",
        "#테스트 데이터 \n",
        "print(\"x_test의 크기: \",x_test.shape) #(sample수, dimension)\n",
        "print(\"y_test의 크기: \",y_test.shape)\n",
        "print(type(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPipGVF9wCDJ"
      },
      "source": [
        "#### -Scailing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk3j8H4qwCDL"
      },
      "outputs": [],
      "source": [
        "#학습 데이터 Scaling\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train_scale = scaler.transform(x_train) # x_train_scale은 numpy ndarray \n",
        "\n",
        "\n",
        "#테스트 데이터 Scaling\n",
        "x_test_scale = scaler.transform(x_test) # x_test_scale은 numpy ndarray \n",
        "\n",
        "\n",
        "# Array-->Tensor\n",
        "x_train_tensor = torch.FloatTensor(x_train_scale)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values) #y_train은 판다스 Series이므로 values를 사용해서 numpy ndarray로 가져오기\n",
        "\n",
        "x_test_tensor = torch.FloatTensor(x_test_scale)\n",
        "y_test_tensor = torch.FloatTensor(y_test.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJ-v2BwwCDM"
      },
      "source": [
        "##### -Batchfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dhs8fqWwCDN",
        "outputId": "18eb189e-25b2-4f77-a56e-a51b550ca412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "torch.Size([354, 13])\n",
            "torch.Size([354])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#학습 데이터 배치화 시키기 \n",
        "train_data = data_utils.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "\n",
        "dataloader = data_utils.DataLoader(train_data, batch_size=354, shuffle=True , drop_last=True)\n",
        "\n",
        "\n",
        "#배치화된 데이터 확인\n",
        "for batch_idx, datas in enumerate(dataloader):\n",
        "    print(batch_idx)\n",
        "    print(datas[0].shape)  # x_train \n",
        "    print(datas[1].shape) # y_train\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tGb08pawCDP"
      },
      "source": [
        "### 2) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6UMvKtHwCDQ"
      },
      "outputs": [],
      "source": [
        "# Modeling\n",
        "class MLP_model(torch.nn.Module):\n",
        "        def __init__(self, input_size, hidden_size ,output_size):\n",
        "            super(MLP_model, self).__init__()\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size = hidden_size\n",
        "            self.output_size = output_size\n",
        "            self.hidden2 = 16\n",
        "            \n",
        "            #MLP이기 때문에 여러 개의 함수 필요 이때 변수 값 주의\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden2)\n",
        "            self.fc3 = torch.nn.Linear(self.hidden2, self.output_size)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            self.sig = torch.nn.Sigmoid()\n",
        "            \n",
        "        def forward(self, x):\n",
        "            fc1 = self.fc1(x)\n",
        "            ac1 = self.relu(fc1)\n",
        "            fc2 = self.fc2(ac1)\n",
        "            ac2 = self.relu(fc2)\n",
        "            fc3 = self.fc3(ac2)\n",
        "\n",
        "\n",
        "            return fc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UgULBkXqwCDT"
      },
      "outputs": [],
      "source": [
        "#Parameter 정의\n",
        "input_dim = 13 #feature 수\n",
        "output_dim = 1\n",
        "hidden_dim = 32 #대체로 2의 배수로 사용\n",
        "learning_rate = 0.001\n",
        "n_epochs = 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtOZYNdkwCDU"
      },
      "outputs": [],
      "source": [
        "#model 생성\n",
        "model = MLP_model(input_size = input_dim, hidden_size = hidden_dim, output_size = output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cYEEIykwCDU"
      },
      "outputs": [],
      "source": [
        "#손실함수 생성\n",
        "criterion = torch.nn.MSELoss()\n",
        "#Optimizer 생성\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDh3KEcwCDV"
      },
      "source": [
        "### 3) Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "IIoPDbOiwCDV",
        "outputId": "8f54bd09-04cf-4dc3-f817-3c95590a946e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:0, Loss_train:589.86, Loss_test:603.99\n",
            "epoch:1, Loss_train:586.17, Loss_test:600.33\n",
            "epoch:2, Loss_train:582.55, Loss_test:596.69\n",
            "epoch:3, Loss_train:578.96, Loss_test:593.05\n",
            "epoch:4, Loss_train:575.36, Loss_test:589.34\n",
            "epoch:5, Loss_train:571.71, Loss_test:585.55\n",
            "epoch:6, Loss_train:567.97, Loss_test:581.61\n",
            "epoch:7, Loss_train:564.11, Loss_test:577.50\n",
            "epoch:8, Loss_train:560.09, Loss_test:573.17\n",
            "epoch:9, Loss_train:555.86, Loss_test:568.57\n",
            "epoch:10, Loss_train:551.37, Loss_test:563.63\n",
            "epoch:11, Loss_train:546.57, Loss_test:558.30\n",
            "epoch:12, Loss_train:541.39, Loss_test:552.48\n",
            "epoch:13, Loss_train:535.76, Loss_test:546.07\n",
            "epoch:14, Loss_train:529.57, Loss_test:538.97\n",
            "epoch:15, Loss_train:522.72, Loss_test:531.03\n",
            "epoch:16, Loss_train:515.08, Loss_test:522.09\n",
            "epoch:17, Loss_train:506.50, Loss_test:511.95\n",
            "epoch:18, Loss_train:496.79, Loss_test:500.39\n",
            "epoch:19, Loss_train:485.75, Loss_test:487.13\n",
            "epoch:20, Loss_train:473.11, Loss_test:471.86\n",
            "epoch:21, Loss_train:458.58, Loss_test:454.20\n",
            "epoch:22, Loss_train:441.81, Loss_test:433.74\n",
            "epoch:23, Loss_train:422.41, Loss_test:410.05\n",
            "epoch:24, Loss_train:399.96, Loss_test:382.74\n",
            "epoch:25, Loss_train:374.08, Loss_test:351.58\n",
            "epoch:26, Loss_train:344.51, Loss_test:316.74\n",
            "epoch:27, Loss_train:311.38, Loss_test:278.92\n",
            "epoch:28, Loss_train:275.30, Loss_test:239.63\n",
            "epoch:29, Loss_train:237.57, Loss_test:201.35\n",
            "epoch:30, Loss_train:200.43, Loss_test:166.91\n",
            "epoch:31, Loss_train:166.56, Loss_test:138.68\n",
            "epoch:32, Loss_train:138.28, Loss_test:117.53\n",
            "epoch:33, Loss_train:116.66, Loss_test:102.49\n",
            "epoch:34, Loss_train:101.08, Loss_test:91.69\n",
            "epoch:35, Loss_train:89.92, Loss_test:83.33\n",
            "epoch:36, Loss_train:81.50, Loss_test:76.27\n",
            "epoch:37, Loss_train:74.67, Loss_test:69.98\n",
            "epoch:38, Loss_train:68.81, Loss_test:64.28\n",
            "epoch:39, Loss_train:63.65, Loss_test:59.12\n",
            "epoch:40, Loss_train:59.04, Loss_test:54.49\n",
            "epoch:41, Loss_train:54.92, Loss_test:50.39\n",
            "epoch:42, Loss_train:51.28, Loss_test:46.82\n",
            "epoch:43, Loss_train:48.09, Loss_test:43.70\n",
            "epoch:44, Loss_train:45.29, Loss_test:41.00\n",
            "epoch:45, Loss_train:42.87, Loss_test:38.68\n",
            "epoch:46, Loss_train:40.76, Loss_test:36.65\n",
            "epoch:47, Loss_train:38.93, Loss_test:34.89\n",
            "epoch:48, Loss_train:37.35, Loss_test:33.37\n",
            "epoch:49, Loss_train:35.98, Loss_test:32.05\n",
            "epoch:50, Loss_train:34.80, Loss_test:30.90\n",
            "epoch:51, Loss_train:33.76, Loss_test:29.88\n",
            "epoch:52, Loss_train:32.85, Loss_test:28.97\n",
            "epoch:53, Loss_train:32.04, Loss_test:28.17\n",
            "epoch:54, Loss_train:31.31, Loss_test:27.44\n",
            "epoch:55, Loss_train:30.65, Loss_test:26.77\n",
            "epoch:56, Loss_train:30.04, Loss_test:26.17\n",
            "epoch:57, Loss_train:29.48, Loss_test:25.62\n",
            "epoch:58, Loss_train:28.98, Loss_test:25.11\n",
            "epoch:59, Loss_train:28.50, Loss_test:24.64\n",
            "epoch:60, Loss_train:28.07, Loss_test:24.20\n",
            "epoch:61, Loss_train:27.66, Loss_test:23.80\n",
            "epoch:62, Loss_train:27.28, Loss_test:23.43\n",
            "epoch:63, Loss_train:26.93, Loss_test:23.09\n",
            "epoch:64, Loss_train:26.60, Loss_test:22.77\n",
            "epoch:65, Loss_train:26.28, Loss_test:22.47\n",
            "epoch:66, Loss_train:25.98, Loss_test:22.18\n",
            "epoch:67, Loss_train:25.69, Loss_test:21.92\n",
            "epoch:68, Loss_train:25.42, Loss_test:21.67\n",
            "epoch:69, Loss_train:25.16, Loss_test:21.43\n",
            "epoch:70, Loss_train:24.90, Loss_test:21.20\n",
            "epoch:71, Loss_train:24.66, Loss_test:20.98\n",
            "epoch:72, Loss_train:24.43, Loss_test:20.77\n",
            "epoch:73, Loss_train:24.20, Loss_test:20.57\n",
            "epoch:74, Loss_train:23.98, Loss_test:20.38\n",
            "epoch:75, Loss_train:23.77, Loss_test:20.20\n",
            "epoch:76, Loss_train:23.56, Loss_test:20.03\n",
            "epoch:77, Loss_train:23.37, Loss_test:19.87\n",
            "epoch:78, Loss_train:23.17, Loss_test:19.71\n",
            "epoch:79, Loss_train:22.98, Loss_test:19.56\n",
            "epoch:80, Loss_train:22.79, Loss_test:19.41\n",
            "epoch:81, Loss_train:22.61, Loss_test:19.26\n",
            "epoch:82, Loss_train:22.43, Loss_test:19.12\n",
            "epoch:83, Loss_train:22.26, Loss_test:18.99\n",
            "epoch:84, Loss_train:22.09, Loss_test:18.86\n",
            "epoch:85, Loss_train:21.92, Loss_test:18.73\n",
            "epoch:86, Loss_train:21.76, Loss_test:18.61\n",
            "epoch:87, Loss_train:21.60, Loss_test:18.48\n",
            "epoch:88, Loss_train:21.45, Loss_test:18.36\n",
            "epoch:89, Loss_train:21.30, Loss_test:18.24\n",
            "epoch:90, Loss_train:21.15, Loss_test:18.13\n",
            "epoch:91, Loss_train:21.00, Loss_test:18.02\n",
            "epoch:92, Loss_train:20.86, Loss_test:17.91\n",
            "epoch:93, Loss_train:20.72, Loss_test:17.80\n",
            "epoch:94, Loss_train:20.58, Loss_test:17.70\n",
            "epoch:95, Loss_train:20.45, Loss_test:17.61\n",
            "epoch:96, Loss_train:20.32, Loss_test:17.52\n",
            "epoch:97, Loss_train:20.19, Loss_test:17.44\n",
            "epoch:98, Loss_train:20.07, Loss_test:17.35\n",
            "epoch:99, Loss_train:19.94, Loss_test:17.27\n",
            "epoch:100, Loss_train:19.82, Loss_test:17.19\n",
            "epoch:101, Loss_train:19.70, Loss_test:17.11\n",
            "epoch:102, Loss_train:19.58, Loss_test:17.04\n",
            "epoch:103, Loss_train:19.46, Loss_test:16.96\n",
            "epoch:104, Loss_train:19.35, Loss_test:16.89\n",
            "epoch:105, Loss_train:19.24, Loss_test:16.82\n",
            "epoch:106, Loss_train:19.13, Loss_test:16.75\n",
            "epoch:107, Loss_train:19.02, Loss_test:16.69\n",
            "epoch:108, Loss_train:18.92, Loss_test:16.63\n",
            "epoch:109, Loss_train:18.82, Loss_test:16.56\n",
            "epoch:110, Loss_train:18.72, Loss_test:16.51\n",
            "epoch:111, Loss_train:18.63, Loss_test:16.45\n",
            "epoch:112, Loss_train:18.53, Loss_test:16.39\n",
            "epoch:113, Loss_train:18.44, Loss_test:16.34\n",
            "epoch:114, Loss_train:18.35, Loss_test:16.28\n",
            "epoch:115, Loss_train:18.26, Loss_test:16.23\n",
            "epoch:116, Loss_train:18.17, Loss_test:16.18\n",
            "epoch:117, Loss_train:18.09, Loss_test:16.13\n",
            "epoch:118, Loss_train:18.00, Loss_test:16.08\n",
            "epoch:119, Loss_train:17.92, Loss_test:16.03\n",
            "epoch:120, Loss_train:17.84, Loss_test:15.98\n",
            "epoch:121, Loss_train:17.76, Loss_test:15.94\n",
            "epoch:122, Loss_train:17.68, Loss_test:15.89\n",
            "epoch:123, Loss_train:17.60, Loss_test:15.84\n",
            "epoch:124, Loss_train:17.52, Loss_test:15.79\n",
            "epoch:125, Loss_train:17.44, Loss_test:15.75\n",
            "epoch:126, Loss_train:17.37, Loss_test:15.70\n",
            "epoch:127, Loss_train:17.29, Loss_test:15.66\n",
            "epoch:128, Loss_train:17.22, Loss_test:15.62\n",
            "epoch:129, Loss_train:17.15, Loss_test:15.57\n",
            "epoch:130, Loss_train:17.08, Loss_test:15.53\n",
            "epoch:131, Loss_train:17.00, Loss_test:15.49\n",
            "epoch:132, Loss_train:16.93, Loss_test:15.45\n",
            "epoch:133, Loss_train:16.86, Loss_test:15.41\n",
            "epoch:134, Loss_train:16.79, Loss_test:15.37\n",
            "epoch:135, Loss_train:16.72, Loss_test:15.33\n",
            "epoch:136, Loss_train:16.64, Loss_test:15.30\n",
            "epoch:137, Loss_train:16.57, Loss_test:15.26\n",
            "epoch:138, Loss_train:16.50, Loss_test:15.22\n",
            "epoch:139, Loss_train:16.43, Loss_test:15.19\n",
            "epoch:140, Loss_train:16.36, Loss_test:15.15\n",
            "epoch:141, Loss_train:16.29, Loss_test:15.12\n",
            "epoch:142, Loss_train:16.22, Loss_test:15.09\n",
            "epoch:143, Loss_train:16.16, Loss_test:15.05\n",
            "epoch:144, Loss_train:16.09, Loss_test:15.02\n",
            "epoch:145, Loss_train:16.02, Loss_test:14.99\n",
            "epoch:146, Loss_train:15.96, Loss_test:14.96\n",
            "epoch:147, Loss_train:15.90, Loss_test:14.93\n",
            "epoch:148, Loss_train:15.83, Loss_test:14.90\n",
            "epoch:149, Loss_train:15.77, Loss_test:14.86\n",
            "epoch:150, Loss_train:15.71, Loss_test:14.83\n",
            "epoch:151, Loss_train:15.65, Loss_test:14.80\n",
            "epoch:152, Loss_train:15.58, Loss_test:14.77\n",
            "epoch:153, Loss_train:15.52, Loss_test:14.74\n",
            "epoch:154, Loss_train:15.47, Loss_test:14.71\n",
            "epoch:155, Loss_train:15.41, Loss_test:14.68\n",
            "epoch:156, Loss_train:15.35, Loss_test:14.65\n",
            "epoch:157, Loss_train:15.29, Loss_test:14.63\n",
            "epoch:158, Loss_train:15.23, Loss_test:14.60\n",
            "epoch:159, Loss_train:15.17, Loss_test:14.57\n",
            "epoch:160, Loss_train:15.12, Loss_test:14.55\n",
            "epoch:161, Loss_train:15.06, Loss_test:14.52\n",
            "epoch:162, Loss_train:15.01, Loss_test:14.49\n",
            "epoch:163, Loss_train:14.95, Loss_test:14.47\n",
            "epoch:164, Loss_train:14.90, Loss_test:14.44\n",
            "epoch:165, Loss_train:14.85, Loss_test:14.42\n",
            "epoch:166, Loss_train:14.80, Loss_test:14.40\n",
            "epoch:167, Loss_train:14.75, Loss_test:14.37\n",
            "epoch:168, Loss_train:14.70, Loss_test:14.35\n",
            "epoch:169, Loss_train:14.65, Loss_test:14.33\n",
            "epoch:170, Loss_train:14.60, Loss_test:14.30\n",
            "epoch:171, Loss_train:14.56, Loss_test:14.28\n",
            "epoch:172, Loss_train:14.51, Loss_test:14.26\n",
            "epoch:173, Loss_train:14.46, Loss_test:14.24\n",
            "epoch:174, Loss_train:14.42, Loss_test:14.22\n",
            "epoch:175, Loss_train:14.37, Loss_test:14.20\n",
            "epoch:176, Loss_train:14.33, Loss_test:14.18\n",
            "epoch:177, Loss_train:14.28, Loss_test:14.16\n",
            "epoch:178, Loss_train:14.24, Loss_test:14.14\n",
            "epoch:179, Loss_train:14.19, Loss_test:14.12\n",
            "epoch:180, Loss_train:14.15, Loss_test:14.10\n",
            "epoch:181, Loss_train:14.11, Loss_test:14.09\n",
            "epoch:182, Loss_train:14.06, Loss_test:14.07\n",
            "epoch:183, Loss_train:14.02, Loss_test:14.05\n",
            "epoch:184, Loss_train:13.98, Loss_test:14.03\n",
            "epoch:185, Loss_train:13.94, Loss_test:14.02\n",
            "epoch:186, Loss_train:13.90, Loss_test:14.00\n",
            "epoch:187, Loss_train:13.86, Loss_test:13.98\n",
            "epoch:188, Loss_train:13.82, Loss_test:13.97\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:189, Loss_train:13.78, Loss_test:13.95\n",
            "epoch:190, Loss_train:13.74, Loss_test:13.93\n",
            "epoch:191, Loss_train:13.71, Loss_test:13.92\n",
            "epoch:192, Loss_train:13.67, Loss_test:13.91\n",
            "epoch:193, Loss_train:13.63, Loss_test:13.89\n",
            "epoch:194, Loss_train:13.60, Loss_test:13.88\n",
            "epoch:195, Loss_train:13.56, Loss_test:13.86\n",
            "epoch:196, Loss_train:13.52, Loss_test:13.85\n",
            "epoch:197, Loss_train:13.49, Loss_test:13.84\n",
            "epoch:198, Loss_train:13.46, Loss_test:13.82\n",
            "epoch:199, Loss_train:13.42, Loss_test:13.81\n",
            "epoch:200, Loss_train:13.39, Loss_test:13.79\n",
            "epoch:201, Loss_train:13.36, Loss_test:13.78\n",
            "epoch:202, Loss_train:13.32, Loss_test:13.77\n",
            "epoch:203, Loss_train:13.29, Loss_test:13.76\n",
            "epoch:204, Loss_train:13.26, Loss_test:13.74\n",
            "epoch:205, Loss_train:13.23, Loss_test:13.73\n",
            "epoch:206, Loss_train:13.20, Loss_test:13.72\n",
            "epoch:207, Loss_train:13.16, Loss_test:13.71\n",
            "epoch:208, Loss_train:13.13, Loss_test:13.70\n",
            "epoch:209, Loss_train:13.10, Loss_test:13.69\n",
            "epoch:210, Loss_train:13.07, Loss_test:13.68\n",
            "epoch:211, Loss_train:13.04, Loss_test:13.67\n",
            "epoch:212, Loss_train:13.01, Loss_test:13.65\n",
            "epoch:213, Loss_train:12.99, Loss_test:13.64\n",
            "epoch:214, Loss_train:12.96, Loss_test:13.63\n",
            "epoch:215, Loss_train:12.93, Loss_test:13.62\n",
            "epoch:216, Loss_train:12.90, Loss_test:13.60\n",
            "epoch:217, Loss_train:12.87, Loss_test:13.59\n",
            "epoch:218, Loss_train:12.84, Loss_test:13.58\n",
            "epoch:219, Loss_train:12.82, Loss_test:13.57\n",
            "epoch:220, Loss_train:12.79, Loss_test:13.56\n",
            "epoch:221, Loss_train:12.76, Loss_test:13.55\n",
            "epoch:222, Loss_train:12.74, Loss_test:13.54\n",
            "epoch:223, Loss_train:12.71, Loss_test:13.53\n",
            "epoch:224, Loss_train:12.69, Loss_test:13.52\n",
            "epoch:225, Loss_train:12.66, Loss_test:13.51\n",
            "epoch:226, Loss_train:12.64, Loss_test:13.50\n",
            "epoch:227, Loss_train:12.61, Loss_test:13.49\n",
            "epoch:228, Loss_train:12.59, Loss_test:13.48\n",
            "epoch:229, Loss_train:12.56, Loss_test:13.47\n",
            "epoch:230, Loss_train:12.54, Loss_test:13.46\n",
            "epoch:231, Loss_train:12.51, Loss_test:13.45\n",
            "epoch:232, Loss_train:12.49, Loss_test:13.44\n",
            "epoch:233, Loss_train:12.46, Loss_test:13.43\n",
            "epoch:234, Loss_train:12.44, Loss_test:13.43\n",
            "epoch:235, Loss_train:12.42, Loss_test:13.42\n",
            "epoch:236, Loss_train:12.40, Loss_test:13.41\n",
            "epoch:237, Loss_train:12.37, Loss_test:13.40\n",
            "epoch:238, Loss_train:12.35, Loss_test:13.39\n",
            "epoch:239, Loss_train:12.33, Loss_test:13.38\n",
            "epoch:240, Loss_train:12.31, Loss_test:13.38\n",
            "epoch:241, Loss_train:12.29, Loss_test:13.37\n",
            "epoch:242, Loss_train:12.27, Loss_test:13.36\n",
            "epoch:243, Loss_train:12.25, Loss_test:13.35\n",
            "epoch:244, Loss_train:12.23, Loss_test:13.34\n",
            "epoch:245, Loss_train:12.21, Loss_test:13.33\n",
            "epoch:246, Loss_train:12.19, Loss_test:13.32\n",
            "epoch:247, Loss_train:12.17, Loss_test:13.31\n",
            "epoch:248, Loss_train:12.15, Loss_test:13.30\n",
            "epoch:249, Loss_train:12.13, Loss_test:13.29\n",
            "epoch:250, Loss_train:12.11, Loss_test:13.29\n",
            "epoch:251, Loss_train:12.09, Loss_test:13.28\n",
            "epoch:252, Loss_train:12.07, Loss_test:13.27\n",
            "epoch:253, Loss_train:12.05, Loss_test:13.26\n",
            "epoch:254, Loss_train:12.03, Loss_test:13.25\n",
            "epoch:255, Loss_train:12.02, Loss_test:13.24\n",
            "epoch:256, Loss_train:12.00, Loss_test:13.23\n",
            "epoch:257, Loss_train:11.98, Loss_test:13.22\n",
            "epoch:258, Loss_train:11.96, Loss_test:13.21\n",
            "epoch:259, Loss_train:11.95, Loss_test:13.21\n",
            "epoch:260, Loss_train:11.93, Loss_test:13.20\n",
            "epoch:261, Loss_train:11.91, Loss_test:13.19\n",
            "epoch:262, Loss_train:11.89, Loss_test:13.18\n",
            "epoch:263, Loss_train:11.88, Loss_test:13.17\n",
            "epoch:264, Loss_train:11.86, Loss_test:13.16\n",
            "epoch:265, Loss_train:11.84, Loss_test:13.16\n",
            "epoch:266, Loss_train:11.83, Loss_test:13.15\n",
            "epoch:267, Loss_train:11.81, Loss_test:13.14\n",
            "epoch:268, Loss_train:11.79, Loss_test:13.13\n",
            "epoch:269, Loss_train:11.78, Loss_test:13.12\n",
            "epoch:270, Loss_train:11.76, Loss_test:13.11\n",
            "epoch:271, Loss_train:11.74, Loss_test:13.10\n",
            "epoch:272, Loss_train:11.73, Loss_test:13.10\n",
            "epoch:273, Loss_train:11.71, Loss_test:13.09\n",
            "epoch:274, Loss_train:11.70, Loss_test:13.08\n",
            "epoch:275, Loss_train:11.68, Loss_test:13.08\n",
            "epoch:276, Loss_train:11.67, Loss_test:13.07\n",
            "epoch:277, Loss_train:11.65, Loss_test:13.06\n",
            "epoch:278, Loss_train:11.64, Loss_test:13.06\n",
            "epoch:279, Loss_train:11.62, Loss_test:13.05\n",
            "epoch:280, Loss_train:11.61, Loss_test:13.04\n",
            "epoch:281, Loss_train:11.59, Loss_test:13.04\n",
            "epoch:282, Loss_train:11.58, Loss_test:13.03\n",
            "epoch:283, Loss_train:11.56, Loss_test:13.02\n",
            "epoch:284, Loss_train:11.55, Loss_test:13.02\n",
            "epoch:285, Loss_train:11.53, Loss_test:13.01\n",
            "epoch:286, Loss_train:11.52, Loss_test:13.01\n",
            "epoch:287, Loss_train:11.51, Loss_test:13.00\n",
            "epoch:288, Loss_train:11.49, Loss_test:12.99\n",
            "epoch:289, Loss_train:11.48, Loss_test:12.99\n",
            "epoch:290, Loss_train:11.46, Loss_test:12.98\n",
            "epoch:291, Loss_train:11.45, Loss_test:12.97\n",
            "epoch:292, Loss_train:11.43, Loss_test:12.97\n",
            "epoch:293, Loss_train:11.42, Loss_test:12.96\n",
            "epoch:294, Loss_train:11.41, Loss_test:12.96\n",
            "epoch:295, Loss_train:11.39, Loss_test:12.95\n",
            "epoch:296, Loss_train:11.38, Loss_test:12.94\n",
            "epoch:297, Loss_train:11.37, Loss_test:12.94\n",
            "epoch:298, Loss_train:11.35, Loss_test:12.93\n",
            "epoch:299, Loss_train:11.34, Loss_test:12.93\n",
            "epoch:300, Loss_train:11.33, Loss_test:12.92\n",
            "epoch:301, Loss_train:11.31, Loss_test:12.91\n",
            "epoch:302, Loss_train:11.30, Loss_test:12.91\n",
            "epoch:303, Loss_train:11.29, Loss_test:12.90\n",
            "epoch:304, Loss_train:11.27, Loss_test:12.90\n",
            "epoch:305, Loss_train:11.26, Loss_test:12.89\n",
            "epoch:306, Loss_train:11.25, Loss_test:12.88\n",
            "epoch:307, Loss_train:11.23, Loss_test:12.88\n",
            "epoch:308, Loss_train:11.22, Loss_test:12.87\n",
            "epoch:309, Loss_train:11.21, Loss_test:12.87\n",
            "epoch:310, Loss_train:11.20, Loss_test:12.86\n",
            "epoch:311, Loss_train:11.18, Loss_test:12.86\n",
            "epoch:312, Loss_train:11.17, Loss_test:12.85\n",
            "epoch:313, Loss_train:11.16, Loss_test:12.84\n",
            "epoch:314, Loss_train:11.15, Loss_test:12.84\n",
            "epoch:315, Loss_train:11.13, Loss_test:12.83\n",
            "epoch:316, Loss_train:11.12, Loss_test:12.83\n",
            "epoch:317, Loss_train:11.11, Loss_test:12.82\n",
            "epoch:318, Loss_train:11.10, Loss_test:12.81\n",
            "epoch:319, Loss_train:11.09, Loss_test:12.81\n",
            "epoch:320, Loss_train:11.07, Loss_test:12.80\n",
            "epoch:321, Loss_train:11.06, Loss_test:12.79\n",
            "epoch:322, Loss_train:11.05, Loss_test:12.79\n",
            "epoch:323, Loss_train:11.04, Loss_test:12.78\n",
            "epoch:324, Loss_train:11.03, Loss_test:12.78\n",
            "epoch:325, Loss_train:11.02, Loss_test:12.77\n",
            "epoch:326, Loss_train:11.00, Loss_test:12.76\n",
            "epoch:327, Loss_train:10.99, Loss_test:12.76\n",
            "epoch:328, Loss_train:10.98, Loss_test:12.75\n",
            "epoch:329, Loss_train:10.97, Loss_test:12.75\n",
            "epoch:330, Loss_train:10.96, Loss_test:12.74\n",
            "epoch:331, Loss_train:10.95, Loss_test:12.74\n",
            "epoch:332, Loss_train:10.94, Loss_test:12.73\n",
            "epoch:333, Loss_train:10.92, Loss_test:12.72\n",
            "epoch:334, Loss_train:10.91, Loss_test:12.72\n",
            "epoch:335, Loss_train:10.90, Loss_test:12.72\n",
            "epoch:336, Loss_train:10.89, Loss_test:12.71\n",
            "epoch:337, Loss_train:10.88, Loss_test:12.70\n",
            "epoch:338, Loss_train:10.87, Loss_test:12.70\n",
            "epoch:339, Loss_train:10.86, Loss_test:12.69\n",
            "epoch:340, Loss_train:10.84, Loss_test:12.69\n",
            "epoch:341, Loss_train:10.83, Loss_test:12.68\n",
            "epoch:342, Loss_train:10.82, Loss_test:12.67\n",
            "epoch:343, Loss_train:10.81, Loss_test:12.67\n",
            "epoch:344, Loss_train:10.80, Loss_test:12.66\n",
            "epoch:345, Loss_train:10.79, Loss_test:12.66\n",
            "epoch:346, Loss_train:10.78, Loss_test:12.65\n",
            "epoch:347, Loss_train:10.77, Loss_test:12.64\n",
            "epoch:348, Loss_train:10.76, Loss_test:12.64\n",
            "epoch:349, Loss_train:10.74, Loss_test:12.63\n",
            "epoch:350, Loss_train:10.73, Loss_test:12.63\n",
            "epoch:351, Loss_train:10.72, Loss_test:12.62\n",
            "epoch:352, Loss_train:10.71, Loss_test:12.62\n",
            "epoch:353, Loss_train:10.70, Loss_test:12.61\n",
            "epoch:354, Loss_train:10.69, Loss_test:12.60\n",
            "epoch:355, Loss_train:10.68, Loss_test:12.60\n",
            "epoch:356, Loss_train:10.67, Loss_test:12.60\n",
            "epoch:357, Loss_train:10.66, Loss_test:12.59\n",
            "epoch:358, Loss_train:10.65, Loss_test:12.58\n",
            "epoch:359, Loss_train:10.64, Loss_test:12.58\n",
            "epoch:360, Loss_train:10.63, Loss_test:12.57\n",
            "epoch:361, Loss_train:10.62, Loss_test:12.57\n",
            "epoch:362, Loss_train:10.61, Loss_test:12.56\n",
            "epoch:363, Loss_train:10.59, Loss_test:12.56\n",
            "epoch:364, Loss_train:10.58, Loss_test:12.55\n",
            "epoch:365, Loss_train:10.57, Loss_test:12.54\n",
            "epoch:366, Loss_train:10.56, Loss_test:12.54\n",
            "epoch:367, Loss_train:10.55, Loss_test:12.53\n",
            "epoch:368, Loss_train:10.54, Loss_test:12.53\n",
            "epoch:369, Loss_train:10.53, Loss_test:12.52\n",
            "epoch:370, Loss_train:10.52, Loss_test:12.52\n",
            "epoch:371, Loss_train:10.51, Loss_test:12.51\n",
            "epoch:372, Loss_train:10.50, Loss_test:12.51\n",
            "epoch:373, Loss_train:10.49, Loss_test:12.50\n",
            "epoch:374, Loss_train:10.48, Loss_test:12.50\n",
            "epoch:375, Loss_train:10.47, Loss_test:12.49\n",
            "epoch:376, Loss_train:10.46, Loss_test:12.49\n",
            "epoch:377, Loss_train:10.45, Loss_test:12.48\n",
            "epoch:378, Loss_train:10.44, Loss_test:12.48\n",
            "epoch:379, Loss_train:10.43, Loss_test:12.47\n",
            "epoch:380, Loss_train:10.42, Loss_test:12.47\n",
            "epoch:381, Loss_train:10.41, Loss_test:12.47\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:382, Loss_train:10.40, Loss_test:12.46\n",
            "epoch:383, Loss_train:10.39, Loss_test:12.46\n",
            "epoch:384, Loss_train:10.38, Loss_test:12.45\n",
            "epoch:385, Loss_train:10.37, Loss_test:12.45\n",
            "epoch:386, Loss_train:10.36, Loss_test:12.45\n",
            "epoch:387, Loss_train:10.35, Loss_test:12.44\n",
            "epoch:388, Loss_train:10.34, Loss_test:12.44\n",
            "epoch:389, Loss_train:10.33, Loss_test:12.43\n",
            "epoch:390, Loss_train:10.32, Loss_test:12.43\n",
            "epoch:391, Loss_train:10.31, Loss_test:12.42\n",
            "epoch:392, Loss_train:10.30, Loss_test:12.42\n",
            "epoch:393, Loss_train:10.29, Loss_test:12.41\n",
            "epoch:394, Loss_train:10.28, Loss_test:12.41\n",
            "epoch:395, Loss_train:10.27, Loss_test:12.40\n",
            "epoch:396, Loss_train:10.26, Loss_test:12.40\n",
            "epoch:397, Loss_train:10.25, Loss_test:12.40\n",
            "epoch:398, Loss_train:10.24, Loss_test:12.39\n",
            "epoch:399, Loss_train:10.23, Loss_test:12.38\n",
            "epoch:400, Loss_train:10.22, Loss_test:12.38\n",
            "epoch:401, Loss_train:10.21, Loss_test:12.38\n",
            "epoch:402, Loss_train:10.20, Loss_test:12.37\n",
            "epoch:403, Loss_train:10.19, Loss_test:12.37\n",
            "epoch:404, Loss_train:10.19, Loss_test:12.36\n",
            "epoch:405, Loss_train:10.18, Loss_test:12.36\n",
            "epoch:406, Loss_train:10.17, Loss_test:12.35\n",
            "epoch:407, Loss_train:10.16, Loss_test:12.35\n",
            "epoch:408, Loss_train:10.15, Loss_test:12.34\n",
            "epoch:409, Loss_train:10.14, Loss_test:12.34\n",
            "epoch:410, Loss_train:10.14, Loss_test:12.33\n",
            "epoch:411, Loss_train:10.13, Loss_test:12.33\n",
            "epoch:412, Loss_train:10.12, Loss_test:12.32\n",
            "epoch:413, Loss_train:10.11, Loss_test:12.32\n",
            "epoch:414, Loss_train:10.10, Loss_test:12.32\n",
            "epoch:415, Loss_train:10.10, Loss_test:12.31\n",
            "epoch:416, Loss_train:10.09, Loss_test:12.31\n",
            "epoch:417, Loss_train:10.08, Loss_test:12.30\n",
            "epoch:418, Loss_train:10.07, Loss_test:12.30\n",
            "epoch:419, Loss_train:10.06, Loss_test:12.30\n",
            "epoch:420, Loss_train:10.06, Loss_test:12.29\n",
            "epoch:421, Loss_train:10.05, Loss_test:12.29\n",
            "epoch:422, Loss_train:10.04, Loss_test:12.28\n",
            "epoch:423, Loss_train:10.03, Loss_test:12.28\n",
            "epoch:424, Loss_train:10.02, Loss_test:12.28\n",
            "epoch:425, Loss_train:10.02, Loss_test:12.27\n",
            "epoch:426, Loss_train:10.01, Loss_test:12.27\n",
            "epoch:427, Loss_train:10.00, Loss_test:12.26\n",
            "epoch:428, Loss_train:9.99, Loss_test:12.26\n",
            "epoch:429, Loss_train:9.99, Loss_test:12.25\n",
            "epoch:430, Loss_train:9.98, Loss_test:12.25\n",
            "epoch:431, Loss_train:9.97, Loss_test:12.24\n",
            "epoch:432, Loss_train:9.96, Loss_test:12.24\n",
            "epoch:433, Loss_train:9.96, Loss_test:12.24\n",
            "epoch:434, Loss_train:9.95, Loss_test:12.23\n",
            "epoch:435, Loss_train:9.94, Loss_test:12.23\n",
            "epoch:436, Loss_train:9.93, Loss_test:12.23\n",
            "epoch:437, Loss_train:9.93, Loss_test:12.22\n",
            "epoch:438, Loss_train:9.92, Loss_test:12.22\n",
            "epoch:439, Loss_train:9.91, Loss_test:12.22\n",
            "epoch:440, Loss_train:9.90, Loss_test:12.21\n",
            "epoch:441, Loss_train:9.89, Loss_test:12.21\n",
            "epoch:442, Loss_train:9.89, Loss_test:12.21\n",
            "epoch:443, Loss_train:9.88, Loss_test:12.20\n",
            "epoch:444, Loss_train:9.87, Loss_test:12.20\n",
            "epoch:445, Loss_train:9.87, Loss_test:12.19\n",
            "epoch:446, Loss_train:9.86, Loss_test:12.19\n",
            "epoch:447, Loss_train:9.85, Loss_test:12.19\n",
            "epoch:448, Loss_train:9.84, Loss_test:12.18\n",
            "epoch:449, Loss_train:9.84, Loss_test:12.18\n",
            "epoch:450, Loss_train:9.83, Loss_test:12.18\n",
            "epoch:451, Loss_train:9.82, Loss_test:12.17\n",
            "epoch:452, Loss_train:9.81, Loss_test:12.17\n",
            "epoch:453, Loss_train:9.81, Loss_test:12.16\n",
            "epoch:454, Loss_train:9.80, Loss_test:12.16\n",
            "epoch:455, Loss_train:9.79, Loss_test:12.16\n",
            "epoch:456, Loss_train:9.78, Loss_test:12.15\n",
            "epoch:457, Loss_train:9.78, Loss_test:12.15\n",
            "epoch:458, Loss_train:9.77, Loss_test:12.15\n",
            "epoch:459, Loss_train:9.76, Loss_test:12.14\n",
            "epoch:460, Loss_train:9.75, Loss_test:12.14\n",
            "epoch:461, Loss_train:9.75, Loss_test:12.13\n",
            "epoch:462, Loss_train:9.74, Loss_test:12.13\n",
            "epoch:463, Loss_train:9.73, Loss_test:12.13\n",
            "epoch:464, Loss_train:9.73, Loss_test:12.12\n",
            "epoch:465, Loss_train:9.72, Loss_test:12.12\n",
            "epoch:466, Loss_train:9.71, Loss_test:12.11\n",
            "epoch:467, Loss_train:9.71, Loss_test:12.11\n",
            "epoch:468, Loss_train:9.70, Loss_test:12.11\n",
            "epoch:469, Loss_train:9.69, Loss_test:12.10\n",
            "epoch:470, Loss_train:9.68, Loss_test:12.10\n",
            "epoch:471, Loss_train:9.68, Loss_test:12.10\n",
            "epoch:472, Loss_train:9.67, Loss_test:12.09\n",
            "epoch:473, Loss_train:9.66, Loss_test:12.09\n",
            "epoch:474, Loss_train:9.65, Loss_test:12.08\n",
            "epoch:475, Loss_train:9.65, Loss_test:12.08\n",
            "epoch:476, Loss_train:9.64, Loss_test:12.08\n",
            "epoch:477, Loss_train:9.63, Loss_test:12.07\n",
            "epoch:478, Loss_train:9.63, Loss_test:12.07\n",
            "epoch:479, Loss_train:9.62, Loss_test:12.06\n",
            "epoch:480, Loss_train:9.61, Loss_test:12.06\n",
            "epoch:481, Loss_train:9.61, Loss_test:12.06\n",
            "epoch:482, Loss_train:9.60, Loss_test:12.05\n",
            "epoch:483, Loss_train:9.59, Loss_test:12.04\n",
            "epoch:484, Loss_train:9.59, Loss_test:12.04\n",
            "epoch:485, Loss_train:9.58, Loss_test:12.03\n",
            "epoch:486, Loss_train:9.57, Loss_test:12.03\n",
            "epoch:487, Loss_train:9.57, Loss_test:12.03\n",
            "epoch:488, Loss_train:9.56, Loss_test:12.02\n",
            "epoch:489, Loss_train:9.55, Loss_test:12.02\n",
            "epoch:490, Loss_train:9.55, Loss_test:12.01\n",
            "epoch:491, Loss_train:9.54, Loss_test:12.01\n",
            "epoch:492, Loss_train:9.53, Loss_test:12.00\n",
            "epoch:493, Loss_train:9.53, Loss_test:12.00\n",
            "epoch:494, Loss_train:9.52, Loss_test:11.99\n",
            "epoch:495, Loss_train:9.51, Loss_test:11.99\n",
            "epoch:496, Loss_train:9.51, Loss_test:11.98\n",
            "epoch:497, Loss_train:9.50, Loss_test:11.97\n",
            "epoch:498, Loss_train:9.49, Loss_test:11.97\n",
            "epoch:499, Loss_train:9.49, Loss_test:11.96\n",
            "epoch:500, Loss_train:9.48, Loss_test:11.96\n",
            "epoch:501, Loss_train:9.47, Loss_test:11.95\n",
            "epoch:502, Loss_train:9.47, Loss_test:11.95\n",
            "epoch:503, Loss_train:9.46, Loss_test:11.94\n",
            "epoch:504, Loss_train:9.46, Loss_test:11.94\n",
            "epoch:505, Loss_train:9.45, Loss_test:11.93\n",
            "epoch:506, Loss_train:9.44, Loss_test:11.93\n",
            "epoch:507, Loss_train:9.44, Loss_test:11.92\n",
            "epoch:508, Loss_train:9.43, Loss_test:11.91\n",
            "epoch:509, Loss_train:9.42, Loss_test:11.91\n",
            "epoch:510, Loss_train:9.42, Loss_test:11.90\n",
            "epoch:511, Loss_train:9.41, Loss_test:11.89\n",
            "epoch:512, Loss_train:9.40, Loss_test:11.89\n",
            "epoch:513, Loss_train:9.40, Loss_test:11.88\n",
            "epoch:514, Loss_train:9.39, Loss_test:11.87\n",
            "epoch:515, Loss_train:9.38, Loss_test:11.87\n",
            "epoch:516, Loss_train:9.38, Loss_test:11.86\n",
            "epoch:517, Loss_train:9.37, Loss_test:11.85\n",
            "epoch:518, Loss_train:9.36, Loss_test:11.85\n",
            "epoch:519, Loss_train:9.36, Loss_test:11.85\n",
            "epoch:520, Loss_train:9.35, Loss_test:11.84\n",
            "epoch:521, Loss_train:9.34, Loss_test:11.83\n",
            "epoch:522, Loss_train:9.34, Loss_test:11.83\n",
            "epoch:523, Loss_train:9.33, Loss_test:11.82\n",
            "epoch:524, Loss_train:9.32, Loss_test:11.82\n",
            "epoch:525, Loss_train:9.32, Loss_test:11.81\n",
            "epoch:526, Loss_train:9.31, Loss_test:11.81\n",
            "epoch:527, Loss_train:9.30, Loss_test:11.80\n",
            "epoch:528, Loss_train:9.30, Loss_test:11.80\n",
            "epoch:529, Loss_train:9.29, Loss_test:11.79\n",
            "epoch:530, Loss_train:9.29, Loss_test:11.78\n",
            "epoch:531, Loss_train:9.28, Loss_test:11.78\n",
            "epoch:532, Loss_train:9.27, Loss_test:11.78\n",
            "epoch:533, Loss_train:9.27, Loss_test:11.77\n",
            "epoch:534, Loss_train:9.26, Loss_test:11.77\n",
            "epoch:535, Loss_train:9.25, Loss_test:11.76\n",
            "epoch:536, Loss_train:9.25, Loss_test:11.75\n",
            "epoch:537, Loss_train:9.24, Loss_test:11.75\n",
            "epoch:538, Loss_train:9.24, Loss_test:11.75\n",
            "epoch:539, Loss_train:9.23, Loss_test:11.74\n",
            "epoch:540, Loss_train:9.22, Loss_test:11.74\n",
            "epoch:541, Loss_train:9.22, Loss_test:11.73\n",
            "epoch:542, Loss_train:9.21, Loss_test:11.73\n",
            "epoch:543, Loss_train:9.20, Loss_test:11.73\n",
            "epoch:544, Loss_train:9.20, Loss_test:11.72\n",
            "epoch:545, Loss_train:9.19, Loss_test:11.72\n",
            "epoch:546, Loss_train:9.19, Loss_test:11.71\n",
            "epoch:547, Loss_train:9.18, Loss_test:11.71\n",
            "epoch:548, Loss_train:9.17, Loss_test:11.70\n",
            "epoch:549, Loss_train:9.17, Loss_test:11.70\n",
            "epoch:550, Loss_train:9.16, Loss_test:11.70\n",
            "epoch:551, Loss_train:9.16, Loss_test:11.69\n",
            "epoch:552, Loss_train:9.15, Loss_test:11.69\n",
            "epoch:553, Loss_train:9.15, Loss_test:11.68\n",
            "epoch:554, Loss_train:9.14, Loss_test:11.68\n",
            "epoch:555, Loss_train:9.14, Loss_test:11.68\n",
            "epoch:556, Loss_train:9.13, Loss_test:11.67\n",
            "epoch:557, Loss_train:9.12, Loss_test:11.67\n",
            "epoch:558, Loss_train:9.12, Loss_test:11.66\n",
            "epoch:559, Loss_train:9.11, Loss_test:11.66\n",
            "epoch:560, Loss_train:9.11, Loss_test:11.66\n",
            "epoch:561, Loss_train:9.10, Loss_test:11.65\n",
            "epoch:562, Loss_train:9.10, Loss_test:11.64\n",
            "epoch:563, Loss_train:9.09, Loss_test:11.64\n",
            "epoch:564, Loss_train:9.08, Loss_test:11.64\n",
            "epoch:565, Loss_train:9.08, Loss_test:11.63\n",
            "epoch:566, Loss_train:9.07, Loss_test:11.62\n",
            "epoch:567, Loss_train:9.07, Loss_test:11.62\n",
            "epoch:568, Loss_train:9.06, Loss_test:11.62\n",
            "epoch:569, Loss_train:9.06, Loss_test:11.61\n",
            "epoch:570, Loss_train:9.05, Loss_test:11.61\n",
            "epoch:571, Loss_train:9.05, Loss_test:11.60\n",
            "epoch:572, Loss_train:9.04, Loss_test:11.60\n",
            "epoch:573, Loss_train:9.03, Loss_test:11.60\n",
            "epoch:574, Loss_train:9.03, Loss_test:11.59\n",
            "epoch:575, Loss_train:9.02, Loss_test:11.58\n",
            "epoch:576, Loss_train:9.02, Loss_test:11.58\n",
            "epoch:577, Loss_train:9.01, Loss_test:11.57\n",
            "epoch:578, Loss_train:9.01, Loss_test:11.56\n",
            "epoch:579, Loss_train:9.00, Loss_test:11.56\n",
            "epoch:580, Loss_train:9.00, Loss_test:11.55\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:581, Loss_train:8.99, Loss_test:11.55\n",
            "epoch:582, Loss_train:8.98, Loss_test:11.54\n",
            "epoch:583, Loss_train:8.98, Loss_test:11.54\n",
            "epoch:584, Loss_train:8.97, Loss_test:11.53\n",
            "epoch:585, Loss_train:8.97, Loss_test:11.53\n",
            "epoch:586, Loss_train:8.96, Loss_test:11.53\n",
            "epoch:587, Loss_train:8.96, Loss_test:11.52\n",
            "epoch:588, Loss_train:8.95, Loss_test:11.52\n",
            "epoch:589, Loss_train:8.95, Loss_test:11.51\n",
            "epoch:590, Loss_train:8.94, Loss_test:11.51\n",
            "epoch:591, Loss_train:8.94, Loss_test:11.51\n",
            "epoch:592, Loss_train:8.93, Loss_test:11.50\n",
            "epoch:593, Loss_train:8.93, Loss_test:11.50\n",
            "epoch:594, Loss_train:8.92, Loss_test:11.50\n",
            "epoch:595, Loss_train:8.92, Loss_test:11.49\n",
            "epoch:596, Loss_train:8.91, Loss_test:11.49\n",
            "epoch:597, Loss_train:8.91, Loss_test:11.49\n",
            "epoch:598, Loss_train:8.90, Loss_test:11.49\n",
            "epoch:599, Loss_train:8.90, Loss_test:11.48\n",
            "epoch:600, Loss_train:8.89, Loss_test:11.48\n",
            "epoch:601, Loss_train:8.89, Loss_test:11.48\n",
            "epoch:602, Loss_train:8.88, Loss_test:11.47\n",
            "epoch:603, Loss_train:8.88, Loss_test:11.47\n",
            "epoch:604, Loss_train:8.87, Loss_test:11.47\n",
            "epoch:605, Loss_train:8.87, Loss_test:11.47\n",
            "epoch:606, Loss_train:8.86, Loss_test:11.46\n",
            "epoch:607, Loss_train:8.86, Loss_test:11.46\n",
            "epoch:608, Loss_train:8.85, Loss_test:11.45\n",
            "epoch:609, Loss_train:8.85, Loss_test:11.45\n",
            "epoch:610, Loss_train:8.84, Loss_test:11.45\n",
            "epoch:611, Loss_train:8.84, Loss_test:11.45\n",
            "epoch:612, Loss_train:8.83, Loss_test:11.44\n",
            "epoch:613, Loss_train:8.83, Loss_test:11.44\n",
            "epoch:614, Loss_train:8.82, Loss_test:11.44\n",
            "epoch:615, Loss_train:8.82, Loss_test:11.43\n",
            "epoch:616, Loss_train:8.81, Loss_test:11.43\n",
            "epoch:617, Loss_train:8.80, Loss_test:11.43\n",
            "epoch:618, Loss_train:8.80, Loss_test:11.42\n",
            "epoch:619, Loss_train:8.79, Loss_test:11.42\n",
            "epoch:620, Loss_train:8.79, Loss_test:11.42\n",
            "epoch:621, Loss_train:8.79, Loss_test:11.41\n",
            "epoch:622, Loss_train:8.78, Loss_test:11.41\n",
            "epoch:623, Loss_train:8.77, Loss_test:11.41\n",
            "epoch:624, Loss_train:8.77, Loss_test:11.41\n",
            "epoch:625, Loss_train:8.77, Loss_test:11.40\n",
            "epoch:626, Loss_train:8.76, Loss_test:11.40\n",
            "epoch:627, Loss_train:8.76, Loss_test:11.40\n",
            "epoch:628, Loss_train:8.75, Loss_test:11.40\n",
            "epoch:629, Loss_train:8.75, Loss_test:11.39\n",
            "epoch:630, Loss_train:8.74, Loss_test:11.39\n",
            "epoch:631, Loss_train:8.74, Loss_test:11.39\n",
            "epoch:632, Loss_train:8.73, Loss_test:11.39\n",
            "epoch:633, Loss_train:8.73, Loss_test:11.38\n",
            "epoch:634, Loss_train:8.72, Loss_test:11.38\n",
            "epoch:635, Loss_train:8.72, Loss_test:11.38\n",
            "epoch:636, Loss_train:8.71, Loss_test:11.37\n",
            "epoch:637, Loss_train:8.71, Loss_test:11.37\n",
            "epoch:638, Loss_train:8.70, Loss_test:11.37\n",
            "epoch:639, Loss_train:8.70, Loss_test:11.37\n",
            "epoch:640, Loss_train:8.69, Loss_test:11.36\n",
            "epoch:641, Loss_train:8.69, Loss_test:11.36\n",
            "epoch:642, Loss_train:8.68, Loss_test:11.36\n",
            "epoch:643, Loss_train:8.68, Loss_test:11.36\n",
            "epoch:644, Loss_train:8.67, Loss_test:11.36\n",
            "epoch:645, Loss_train:8.67, Loss_test:11.35\n",
            "epoch:646, Loss_train:8.66, Loss_test:11.35\n",
            "epoch:647, Loss_train:8.66, Loss_test:11.35\n",
            "epoch:648, Loss_train:8.65, Loss_test:11.35\n",
            "epoch:649, Loss_train:8.65, Loss_test:11.34\n",
            "epoch:650, Loss_train:8.64, Loss_test:11.34\n",
            "epoch:651, Loss_train:8.64, Loss_test:11.34\n",
            "epoch:652, Loss_train:8.64, Loss_test:11.33\n",
            "epoch:653, Loss_train:8.63, Loss_test:11.33\n",
            "epoch:654, Loss_train:8.63, Loss_test:11.33\n",
            "epoch:655, Loss_train:8.62, Loss_test:11.33\n",
            "epoch:656, Loss_train:8.62, Loss_test:11.33\n",
            "epoch:657, Loss_train:8.61, Loss_test:11.32\n",
            "epoch:658, Loss_train:8.61, Loss_test:11.32\n",
            "epoch:659, Loss_train:8.60, Loss_test:11.32\n",
            "epoch:660, Loss_train:8.60, Loss_test:11.32\n",
            "epoch:661, Loss_train:8.59, Loss_test:11.31\n",
            "epoch:662, Loss_train:8.59, Loss_test:11.31\n",
            "epoch:663, Loss_train:8.58, Loss_test:11.31\n",
            "epoch:664, Loss_train:8.58, Loss_test:11.31\n",
            "epoch:665, Loss_train:8.57, Loss_test:11.30\n",
            "epoch:666, Loss_train:8.57, Loss_test:11.30\n",
            "epoch:667, Loss_train:8.57, Loss_test:11.30\n",
            "epoch:668, Loss_train:8.56, Loss_test:11.30\n",
            "epoch:669, Loss_train:8.56, Loss_test:11.29\n",
            "epoch:670, Loss_train:8.55, Loss_test:11.29\n",
            "epoch:671, Loss_train:8.55, Loss_test:11.29\n",
            "epoch:672, Loss_train:8.54, Loss_test:11.29\n",
            "epoch:673, Loss_train:8.54, Loss_test:11.28\n",
            "epoch:674, Loss_train:8.53, Loss_test:11.28\n",
            "epoch:675, Loss_train:8.53, Loss_test:11.28\n",
            "epoch:676, Loss_train:8.52, Loss_test:11.28\n",
            "epoch:677, Loss_train:8.52, Loss_test:11.27\n",
            "epoch:678, Loss_train:8.51, Loss_test:11.27\n",
            "epoch:679, Loss_train:8.51, Loss_test:11.27\n",
            "epoch:680, Loss_train:8.51, Loss_test:11.26\n",
            "epoch:681, Loss_train:8.50, Loss_test:11.26\n",
            "epoch:682, Loss_train:8.50, Loss_test:11.26\n",
            "epoch:683, Loss_train:8.49, Loss_test:11.26\n",
            "epoch:684, Loss_train:8.49, Loss_test:11.25\n",
            "epoch:685, Loss_train:8.48, Loss_test:11.25\n",
            "epoch:686, Loss_train:8.48, Loss_test:11.25\n",
            "epoch:687, Loss_train:8.47, Loss_test:11.24\n",
            "epoch:688, Loss_train:8.47, Loss_test:11.24\n",
            "epoch:689, Loss_train:8.46, Loss_test:11.24\n",
            "epoch:690, Loss_train:8.46, Loss_test:11.23\n",
            "epoch:691, Loss_train:8.45, Loss_test:11.23\n",
            "epoch:692, Loss_train:8.45, Loss_test:11.22\n",
            "epoch:693, Loss_train:8.44, Loss_test:11.22\n",
            "epoch:694, Loss_train:8.44, Loss_test:11.21\n",
            "epoch:695, Loss_train:8.44, Loss_test:11.21\n",
            "epoch:696, Loss_train:8.43, Loss_test:11.20\n",
            "epoch:697, Loss_train:8.43, Loss_test:11.20\n",
            "epoch:698, Loss_train:8.42, Loss_test:11.19\n",
            "epoch:699, Loss_train:8.42, Loss_test:11.19\n",
            "epoch:700, Loss_train:8.41, Loss_test:11.19\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "# 값이 수렴할 때까지 epoch 조정해가면서 training 해줘야 함\n",
        "# layer가 쌓일수록 overfitting\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(n_epochs+1):\n",
        "    \n",
        "    #Batch 학습\n",
        "    for idx, (x_batch, y_batch) in enumerate(dataloader):\n",
        "        optimizer.zero_grad() #gradient 값 초기화\n",
        "        y_pred = model(x_batch)\n",
        "        y_pred = y_pred.reshape(-1)\n",
        "        loss_train = criterion(y_pred, y_batch)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #Validation\n",
        "        y_test_pred = model(x_test_tensor)\n",
        "        y_test_pred = y_test_pred.reshape(-1) #차원을 맞춰야 loss값을 정확히 알 수 있음\n",
        "        loss_test = criterion(y_test_pred, y_test_tensor)\n",
        "\n",
        "    train_loss.append(loss_train.item())\n",
        "    test_loss.append(loss_test.item())\n",
        "    print(\"epoch:{}, Loss_train:{:.2f}, Loss_test:{:.2f}\".format( epoch, train_loss[-1], test_loss[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT2bwap7wCDW",
        "outputId": "03c360a2-15d7-413b-bd2d-e9f78d41ef80"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loss 값 plot\n",
        "plt.figure()\n",
        "plt.plot(train_loss, label='train')\n",
        "plt.plot(test_loss, label='test')\n",
        "plt.title('Model loss')\n",
        "plt.legend(loc= 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUa0rTcBwCDW"
      },
      "source": [
        "### <Scaling>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXtMr0WpwCDW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chap2-1)_Regression_with_MLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}